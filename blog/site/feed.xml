<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
<<<<<<< HEAD
    <title>tanner geo Blog</title>
    <description>tannergeo is passionate about computer science, web development and everything geospatial!  We are super greatful for all of the great tutorials and blog posts other have created.  We want to help give back by posting our  experiences and tutorials on new technologies.
</description>
    <link>http://tannergeo.com/~joshuatanner/tannergeo//</link>
    <atom:link href="http://tannergeo.com/~joshuatanner/tannergeo//feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 08 Jan 2015 20:26:44 -0800</pubDate>
    <lastBuildDate>Thu, 08 Jan 2015 20:26:44 -0800</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
=======
    <title>TannerGeo Blog</title>
    <description>TannerGeo is passionate about computer science, web development and everything geospatial!  We appreciate the helpful tutorials and blog posts others have created.  We would like to give back as well by posting our  experiences and tutorials on new technologies.
</description>
    <link>http://tannergeo.com/blog/site//</link>
    <atom:link href="http://tannergeo.com/blog/site//feed.xml" rel="self" type="application/rss+xml" />
<<<<<<< HEAD
<<<<<<< HEAD
    <pubDate>Sun, 11 Jan 2015 18:47:48 -0800</pubDate>
    <lastBuildDate>Sun, 11 Jan 2015 18:47:48 -0800</lastBuildDate>
    <generator>Jekyll v2.5.2</generator>
>>>>>>> gh-pages
=======
    <pubDate>Mon, 02 Mar 2015 21:38:34 -0800</pubDate>
    <lastBuildDate>Mon, 02 Mar 2015 21:38:34 -0800</lastBuildDate>
=======
    <pubDate>Fri, 04 Sep 2015 18:46:46 -0700</pubDate>
    <lastBuildDate>Fri, 04 Sep 2015 18:46:46 -0700</lastBuildDate>
>>>>>>> gh-pages
    <generator>Jekyll v2.5.3</generator>
>>>>>>> gh-pages
    
      <item>
        <title>Spatial Version Control with GeoGig</title>
        <description>&lt;p&gt;There are a lot of concepts that are core to the programming and development communities that have yet to manifest themselves (to the masses) in the GIS world.  One of them is version control.&lt;/p&gt;

&lt;h4 id=&quot;basics&quot;&gt;Basics&lt;/h4&gt;

&lt;p&gt;If you have ever used git or svn, you know of it’s usefulness in maintaining a movable history of the ‘state’ of your code or other files.  If you have never heard of version control, it is simply a system that keeps track of changes and allows you to ‘roll-back’ if needed.  You can visualize these changes (additions and subtractions) and provide readable comments with every &lt;em&gt;commit&lt;/em&gt;.  It also provides methods to resolve &lt;em&gt;conflicts&lt;/em&gt; if multiple changes contradict each other.&lt;/p&gt;

&lt;p&gt;I one had a colleague who explained version control very elegantly.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;‘copy and paste is the one true evil.’ - colleague/friend&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To be honest, I use this all the time when I try to explain the usefulness of version control.  When you copy and paste, you now have two completely separate datasets that know nothing about each other.  When one is updated, the other falls behind and trying to keep them smartly in sync can become futile.  With version control, we can checkout a copy, make changes, and commit them back to the remote repository.  If updates need to be rolled back, we can easily revert them.&lt;/p&gt;

&lt;h4 id=&quot;why-gis-needs-it&quot;&gt;Why GIS Needs It&lt;/h4&gt;
&lt;p&gt;Technically, Esri has been using versioning in enterprise geodatabase solutions for some time now.  It allows concurrent editing of a geodatabase, conflict management, and the ability to revert to a previous state. However, it requires complex setup and the need to be in the Esri environment.&lt;/p&gt;

&lt;p&gt;I’ll be the first to admit my working project folders usually have about 10+ versions of the same dataset.  GIS analysts are always worried about needing to revert their edits or make a backup in the event of bad edits.  When I worked for the City of Charleston, most of my work was with the planning department.  My ‘final changes’ were never actually final and I would gulp when I heard the phrase “let’s go back to the version we used for the last city council meeting…”.  If your like me, you have a folder that looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bike_route_01152014.shp
bike_route_01202014.shp
bike_route_proposal1.shp
bike_route_final.shp
bike_route_final2.shp
bike_route_final_for_real.shp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a one simple example of why we need version control… badly.&lt;/p&gt;

&lt;p&gt;Another reason is making sure your team is always working with the latest data.  Even if everybody is working on a local clone of a spatial dataset, they could run a quick &lt;em&gt;pull&lt;/em&gt; command at anytime and retrieve the most up to date data.&lt;/p&gt;

&lt;h4 id=&quot;why-not-just-use-git&quot;&gt;Why Not Just Use Git?&lt;/h4&gt;
&lt;p&gt;Current version control systems like git work great for text.  The flawlessly track what text was added or removed, and this provides the information we need to understand changes.  Spatial data is binary, and although current systems will track the changes, it doesn’t provide us with much insight into what changed.  If we wanted to understand what changes were made in our latest commit, we would only see that a bunch of 1’s and 0’s changed… not much help.  What we need to see is how many features were added, modified, or removed and on which dataset.&lt;/p&gt;

&lt;h4 id=&quot;geogig-background&quot;&gt;Geogig Background&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;http://geogig.org/&quot;&gt;GeoGig&lt;/a&gt; (previouly GeoGit) is a distributed version control system (DVCS) for geospatial data.  The ‘distributed’ term is important and follows closer to the architecture of Git than SVN (google git vs svn for more info).  The software is open source, and currently maintained by the &lt;a href=&quot;https://www.locationtech.org/projects/technology.geogig&quot;&gt;Eclipse Foundation&lt;/a&gt; and formerly maintained by BoundlessGeo.  A disclaimer on the GeoGig site states that is in ‘development’ and considered unstable.  In my short experience with the software, it worked great, but I would not yet use this in a full production environment.&lt;/p&gt;

&lt;p&gt;I would love to see &lt;a href=&quot;http://geogig.org/&quot;&gt;GeoGig&lt;/a&gt; developed to be stable enough to use as a reliable tool for spatial data version control.&lt;/p&gt;

&lt;h4 id=&quot;geogig-sample-workflow&quot;&gt;GeoGig Sample Workflow&lt;/h4&gt;
&lt;p&gt;I would recommend following the &lt;a href=&quot;http://geogig.org/#install&quot;&gt;Getting Started&lt;/a&gt; guide for downloading and getting up and running with GeoGig.  The following workflow will demonstrate the following scenario:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You are working in a team environment editing bike routes.  The shapefile needs to be accessible on your local computer for when you are editing in the field and taking work home.  You also need to push your changes to a shared network drive and pull down changes that other editors are doing.  I will assume you are working in a windows environment.&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;On your (S:) shared drive, initialize a GeoGig repository.  This is where your master repo will live, and will act like a library for users to check out and back in their edits.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; S:master_routes&amp;gt; geogig init
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This will create a .geogig directory that will track all of your changes, among other things.  You will need to import your bike_routes shapefile which can be anywhere.  For this example I will assume it’s in the same folder as your newly created GeoGig repository.  Think of this step as putting it into a warehouse.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; S:master_routes&amp;gt; geogig shp import ./bike_routes.shp 
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now that it is in the warehouse, you need add it to a &lt;em&gt;loading dock&lt;/em&gt; - like location before committing it entirely.  Only changes will be added, and in our initial case will be all of our existing features in bike_routes.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; S:master_routes&amp;gt; geogig add 
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is the equivalent of ‘add all changes’ to our loading dock to be shipped from the warehouse.  Next we will commit our added changes, which is like putting it on the truck.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; S:master_routes&amp;gt; geogig commit -m &quot;Add initial commit of bike routes&quot;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The -m trigger allows us to add a message with our commit.  This is helpful to make each commit human readable and easier to use.  Now we want all of our users/editors of this repository to pull down a working repo to their local machines.  For this, our (S:) shared drive will act as a remote repository.  From our (C:) local drive, we can add our connection:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; C:local_bike_routes&amp;gt; geogig clone S:/bike_routes/
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The clone command clones our master repository and automatically creates a remote tracking branch.  In essence, it is a clone that knows how to access it’s master.  It can pull down any changes done to, or push changes up to the master.  Our master remote branch is automatically named &lt;em&gt;origin&lt;/em&gt;.  If you look in your directory you might be confused.  There bike_routes shapefile is nowhere to be found, only a .geogig directory?!?! With GeoGig, everything is stored in the database within the .geogig repository.  If you want to export the shapefile to work on, you will have to do this implicitly:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; C:local_bike_routes&amp;gt; geogig shp export HEAD:bike_routes ./bike_routes.shp
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This exports out a shapefile for you to work on.  You can also export the data out in various different formats (geojson, postgres, spatialite, etc), which can be a great tool in itself.  Now that you have a local shapefile to edit, you can make the changes you need in whatever software you are using (qgis, arcgis, etc).  Once you are ready to push the changes up to the master repository, you will have to use a couple command line steps.  You will need to: import the shapefile into your local repo, stage your edits (loading dock), and commit them (put them on the truck).  Once everything is committed in your local geogig repository, you will push the changes to your remote (origin) branch.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; C:local_bike_routes&amp;gt; geogig shp import ./bike_routes.shp -d bike_routes
 C:local_bike_routes&amp;gt; geogig add
 C:local_bike_routes&amp;gt; geogig commit -m &quot;Add new rutledge ave bike route&quot;
 C:local_bike_routes&amp;gt; geogig push origin master
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We have now updated not only our own bike_routes feature, but also the feature on our S: drive.  All our other editors working on their own local repositories can pull in these changes, and view your comments in the log.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; C:local_bike_routes&amp;gt; geogig pull origin master
 C:local_bike_routes&amp;gt; geogig log
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;discussion&quot;&gt;Discussion&lt;/h4&gt;

&lt;p&gt;I tried this workflow on a dataset that I need to continually update and share (even through I warned you not to use it in production).  It worked without error and organized my process ten-fold.  Below are things I liked, and some I didn’t.&lt;/p&gt;

&lt;p&gt;Positive:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I liked that it inherits from git.  Most of the commands are the same, although the workflow is slightly different.  This makes the learning curve very minimal for those already familiar with git.&lt;/li&gt;
  &lt;li&gt;The distributed architecture works well for geospatial data&lt;/li&gt;
  &lt;li&gt;Being able to export to different formats is great.&lt;/li&gt;
  &lt;li&gt;The documentation is clear and concise.&lt;/li&gt;
  &lt;li&gt;Fast.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Negative:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No support for file geodatabase.  This means you will have to export to a shapefile to use this utility.&lt;/li&gt;
  &lt;li&gt;I don’t like that I have to import my data before I add and commit it.  Importing and then adding seems redundant and complicates the process.  This part is different from git.&lt;/li&gt;
  &lt;li&gt;This may be lazy, but typing git is easier than typing geogig.  I typed it a lot in my workflow and it would be easier if it defaulted to gig or something simple.  Again, I could just be lazy and this could probably be changed easily.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It would be also nice to see a GitHub-like solution for publishing and collaborating on spatial data.  Overall I liked it and will be using this a lot more in my current work.  I encourage both the use and further development of GeoGig to create a stable solution for geospatial data version control.&lt;/p&gt;

&lt;h4 id=&quot;extras&quot;&gt;Extras&lt;/h4&gt;

&lt;p&gt;GeoGig has many extras that I didn’t cover in this article that I think others should check out.  Here are a few:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://geogig.org/docs/interaction/osm.html&quot;&gt;Importing OSM Data into GeoGig&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://geogig.org/docs/interaction/console.html&quot;&gt;The GeoGig Console&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://geogig.org/docs/interaction/geoserver_ui.html&quot;&gt;Using GeoGig with GeoServer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://geogig.org/docs/interaction/web-api.html&quot;&gt;Build in Web Server and API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I especially liked the GeoGig console.  This allowed me to more easily write a batch script that exported the latest shapefiles our of my master repository and output a log of all recent commits.&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;#&lt;/h1&gt;

&lt;p&gt;Other Links:
&lt;a href=&quot;http://www.spatiallyadjusted.com/gis-version-control/&quot;&gt;James Fee Article on GIS Version Control&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 04 Sep 2015 00:00:00 -0700</pubDate>
        <link>http://tannergeo.com/blog/site//geogig/git/versioning/gis/data/2015/09/04/Spatial-Version-Control-with-GeoGig.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//geogig/git/versioning/gis/data/2015/09/04/Spatial-Version-Control-with-GeoGig.html</guid>
        
        
        <category>geogig</category>
        
        <category>git</category>
        
        <category>versioning</category>
        
        <category>gis</category>
        
        <category>data</category>
        
      </item>
    
      <item>
        <title>National Parks Map and Image Gallery</title>
        <description>&lt;p&gt;This application and its accompanying tutorial are our tribute to National Park Week (Apr 18-26).  A working knowledge of &lt;a href=&quot;http://www.qgis.org/en/site/&quot;&gt;QGIS&lt;/a&gt;, &lt;a href=&quot;http://leafletjs.com/&quot;&gt;Leaflet&lt;/a&gt;, &lt;a href=&quot;http://turfjs.org/&quot;&gt;Turf.js&lt;/a&gt;, &lt;a href=&quot;https://jquery.com/&quot;&gt;jQuery&lt;/a&gt;, &lt;a href=&quot;http://getbootstrap.com/&quot;&gt;Bootstrap&lt;/a&gt;, &lt;a href=&quot;http://lokeshdhakar.com/projects/lightbox2/&quot;&gt;Lightbox&lt;/a&gt;, and &lt;a href=&quot;https://www.flickr.com/services/api/&quot;&gt;Flickr API&lt;/a&gt; are required to build out this application using this tutorial.&lt;/p&gt;

&lt;h6 id=&quot;data-acquisition-and-conversiongetting-required-modules&quot;&gt;Data Acquisition and Conversion/Getting Required Modules&lt;/h6&gt;

&lt;p&gt;To get started, the first step is to download the &lt;a href=&quot;https://catalog.data.gov/dataset/national-park-boundariesf0a4c&quot;&gt;national parks boundaries dataset&lt;/a&gt; and save to your working directory. Load the shapefile into QGIS to view and, in this case, simplify geometries due to the file size.  To accomplish that in QGIS, select Vector -&amp;gt; Geometry Tools -&amp;gt; Simplify Geometries.&lt;/p&gt;

&lt;p&gt;Next, using command line and in our working directory, convert the shapefile to GeoJSON using &lt;a href=&quot;http://www.gdal.org/&quot;&gt;GDAL&lt;/a&gt;.  In this case we entered:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ogr2ogr -f GeoJSON &amp;lt;output geojson&amp;gt; &amp;lt;input shapefile&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create the application, we will define our dependencies in our &lt;em&gt;package.json&lt;/em&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&quot;dependencies&quot; : {
   	&quot;turf&quot; : &quot;*&quot;,
   	&quot;leaflet&quot; : &quot;*&quot;,
   	&quot;bootstrap&quot; : &quot;*&quot;,
   	&quot;jquery&quot; : &quot;*&quot;
  	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, using command line, use npm to add the appropriate packages locally.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will see your packages in a new directory called &lt;em&gt;node_modules&lt;/em&gt; inside your project folder.&lt;/p&gt;

&lt;p&gt;In your &lt;em&gt;.gitignore&lt;/em&gt; file, add node_modules, and they will be left out when pushing to your &lt;a href=&quot;https://github.com/&quot;&gt;GitHub&lt;/a&gt; repo.&lt;/p&gt;

&lt;h6 id=&quot;building-the-application&quot;&gt;Building the Application&lt;/h6&gt;

&lt;p&gt;Below is an outline of the steps taken to build out this application, along with some code and image samples.  Dig further into the source code for more information.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;add a Leaflet basemap&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  var map,           // main appliation map
      basemap;       // leaflet basemap layer
		
  var app_config = {
  basemap : &quot;http://{s}.tiles.wmflabs.org/bw-mapnik/{z}/{x}/{y}.png&quot;,
  basemapAttribution : &quot;&amp;amp;copy; &amp;lt;a href=\&quot;http://www.openstreetmap.org/copyright\&quot;&amp;gt;OpenStreetMap&amp;lt;/a&amp;gt;&quot;
  };
	
  // create map and add tiled basemap
  map = L.map(&#39;map&#39;).setView([40.12, -98.57], 4);
  basemap = L.tileLayer(app_config.basemap, {
  attribution: app_config.basemapAttribution
  });
  basemap.addTo(map);
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;filter features to exclude geometries that aren’t parks (rivers, monuments, etc.)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;setup GeoJSON layer style&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  var np_boundaries, // raw geojson
  	np_geo;        // leaflet geojson layer
	
  // filter geojson to only parks
  np_boundaries.features = $.map(np_boundaries.features, function (val, i){
      if(val.properties[&#39;UNIT_TYPE&#39;] == &#39;National Park&#39;) {
          return val;
      }
  });
	
  function style(feature) {
      return {
          weight : 1,
          color : &#39;green&#39;,
          fillColor : &#39;green&#39;
      }
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;setup Flickr configuration&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  // Flickr Configuration
  var flickr_config = {
  maxNumImages : 4, // max number of images to request for each park
  url : &quot;https://api.flickr.com/services/rest&quot;,
  key : &amp;lt;your flickr-provided key&amp;gt;
  };
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;add GeoJSON to map along with initial Flickr images&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  /* GEOJSON LAYER */
  np_geo = L.geoJson(np_boundaries, {
      style : style,
      onEachFeature : function (f, l) {
          var pn =  f.properties[&#39;UNIT_NAME&#39;] + &quot; National Park&quot;;
          l.bindPopup(
              &quot;&amp;lt;h3&amp;gt;&quot; + pn + &quot;&amp;lt;/h3&amp;gt;&quot; +
              &quot;&amp;lt;a href=&#39;http://en.wikipedia.org/wiki/&quot; + pn.split(&quot; &quot;).join(&quot;_&quot;) + 
      &quot;&#39; target=&#39;_blank&#39;&amp;gt;Learn more about this National Park&amp;lt;/a&amp;gt;&quot;, {
              autoPan : false
          });

      }
  });
  np_geo.addTo(map);
  getImages(); // make inital call once
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/natl-park-gallery-1.png&quot; alt=&quot;National Park Gallery Image 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;add events for extent change&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  // setup map events 
  map.on(&#39;moveend&#39;, function (e) {
      getImages();
  });
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;acquire bounding boxes and park name tags in preparation for Flickr API calls (uses Turf.js)&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  var queue = []; // placeholder if need for queue and digest management
  var cached = {}; // cache park images to limit api calls
  var current_gallery = {}; // images and details in current extent
  var current_req_length = 0;
  var queue_length = 0;
	
  function callFlickr(f, parkName) {
      var bbox = turf.extent(f.geometry).join(&quot;,&quot;);
      var tags = parkName.split(&quot; &quot;);
      tags.push(&quot;park&quot;);
      tags.push(&quot;national&quot;);
      tags = tags.join(&quot;,&quot;);
      $.ajax({
          url : flickr_config.url,
          dataType : &#39;json&#39;,
          data : {
              api_key : flickr_config.key,
              tags : tags,
              bbox : bbox,
              format : &#39;json&#39;,
              method : &#39;flickr.photos.search&#39;,
              per_page : flickr_config.maxNumImages,
              nojsoncallback : 1
          }
      }).done(function (resp) {
          queue_length += 1;
          if(resp.stat !== &quot;ok&quot;) {
              if(queue_length == current_req_length) {
                  buildHtml(parkName)
              }
              return;
          }
          var photos = [];
          $.each(resp.photos.photo, function (i, v) {
              photos.push(&quot;https://farm&quot; + v.farm + &quot;.staticflickr.com/&quot; + v.server + &quot;/&quot; + v.id + &quot;_&quot; + v.secret + &quot;_n.jpg&quot;);
          });
          // add to cache
          cached[parkName] = photos;
          current_gallery[parkName] = photos;
          if(queue_length == current_req_length) {
              buildHtml(parkName);
          }
      });
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;get current extent and clear current gallery&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;calculate the # of matches for image queue and make Flickr API calls&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  function getImages() {
      // get current extent
      var mapBounds = map.getBounds();
      var centroid;
      // clear current gallery
      current_gallery = {};
      current_req_length = 0;
      queue_length = 0;
      // calculate number of matches for queue
      $.each(np_geo.getLayers(), function (i, l) {
          // get centroid of layer
          centroid = turf.centroid(l.feature);
          var latlng = L.latLng([centroid.geometry.coordinates[1], centroid.geometry.coordinates[0]]);
          if(mapBounds.contains(latlng)) {
              current_req_length += 1;
          }
      });
      // Make flickr calls
      $.each(np_geo.getLayers(), function (i, l) {
          // get centroid of layer
          centroid = turf.centroid(l.feature);
          var latlng = L.latLng([centroid.geometry.coordinates[1], centroid.geometry.coordinates[0]]);
          if(mapBounds.contains(latlng)) {
              if(l.feature.properties[&#39;UNIT_NAME&#39;] in cached) {
                  // push existing parks with images
                  current_gallery[l.feature.properties[&#39;UNIT_NAME&#39;]] = cached[l.feature.properties[&#39;UNIT_NAME&#39;]];
                  queue_length += 1;
                  if(queue_length == current_req_length) {
                      buildHtml(l.feature.properties[&#39;UNIT_NAME&#39;]);
                  }
              } else {
                  callFlickr(l.feature, l.feature.properties[&#39;UNIT_NAME&#39;]);
              }
          }
      });
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;add events for mouseover, mouseout, and click (uses Lightbox)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;build html for image carousel&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  function highlightStyle() {
      return {
          weight : 4,
          color : &#39;red&#39;,
          fillColor : &#39;red&#39;
      }
  }
	
  function highlightPark(park) {
      var park = park;
      $.each(np_geo.getLayers(), function (idx, layer) {
          if (layer.feature.properties[&#39;UNIT_NAME&#39;] == park) {
              layer.setStyle(highlightStyle());
              layer.openPopup();
          } else {
              layer.setStyle(style());
          }
      })
  }

  // build html for image carousel
  function buildHtml() {
      var gallery_items;
      var gallery_container = $(&quot;&amp;lt;div /&amp;gt;&quot;, {
          id : &#39;owl-slider&#39;,
          class : &#39;own-carousel&#39;
      });
      var list = [];
      $.each(current_gallery, function (park, photos) {
          list.push(&quot;&amp;lt;a class=&#39;park-item&#39; href=&#39;&#39; target=&#39;_blank&#39;&amp;gt;&quot; + park + &quot;&amp;lt;/a&amp;gt;&quot;);
          $.each(photos, function (idx, photo) {
              var div = $(&quot;&amp;lt;div /&amp;gt;&quot;, {
                  class : &#39;thumbnail&#39;
              });

              div.on(&#39;mouseover&#39;, function (e) {
                  highlightPark(park);
              });
              div.on(&#39;mouseout&#39;, function (e) {
                  $.each(np_geo.getLayers(), function (i, l) {
                      l.setStyle(style());
                  });
              });

              var title = $(&quot;&amp;lt;span /&amp;gt;&quot;, {
                  html : &quot;&amp;lt;h2&amp;gt;&amp;lt;span class=&#39;glyphicon glyphicon-globe inverse&#39;&amp;gt;&amp;lt;/span&amp;gt;  &quot; + park + &quot; National Park&amp;lt;/h2&amp;gt;&quot;,
                  class : &#39;parkname-title&#39;
              });

              var fullPhoto = photo.replace(&quot;n.jpg&quot;, &quot;b.jpg&quot;);
	
              var link = $(&quot;&amp;lt;a /&amp;gt;&quot;, {
                  &quot;data-lightbox&quot; : &quot;image-&quot; + idx,
                  &quot;data-title&quot; : park + &quot; (courtesy of Flickr)&quot;,
                  href : fullPhoto
              });

              var img = $(&quot;&amp;lt;img /&amp;gt;&quot;, {
                  src : photo
              });

              link.append(img);
              div.append(link);
              div.append(title);
              gallery_container.append(div);
          });
      });

      list = list.join(&#39; | &#39;);
      $(&quot;#parkslist&quot;).html(list);
      $(&quot;.park-item&quot;).on(&#39;click&#39;, function (e) {
          e.preventDefault();
          highlightPark(e.target.text);
      })
      $(&quot;#slider&quot;).empty();
      $(&quot;#slider&quot;).append(gallery_container);
      $(&quot;#owl-slider&quot;).owlCarousel({
          margin : 10,
          autoWidth : true,
          stagePadding : 50
      });
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/natl-park-gallery-2.png&quot; alt=&quot;National Park Gallery Image 2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;add style with Bootstrap and dynamic sizing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/natl-park-gallery-4.png&quot; alt=&quot;National Park Gallery Image 4&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;finally, add sidebar content and style&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;http://tannerjt.github.io/natl-park-gallery/&quot;&gt;View Application&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tannerjt/natl-park-gallery&quot;&gt;View Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;What it looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/natl-park-gallery-5.png&quot; alt=&quot;National Park Gallery Image 5&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 23 Apr 2015 00:00:00 -0700</pubDate>
        <link>http://tannergeo.com/blog/site//leaflet/qgis/turfjs/lightbox/flickr_api/data.gov/2015/04/23/National-Parks-Map-and-Image-Gallery.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//leaflet/qgis/turfjs/lightbox/flickr_api/data.gov/2015/04/23/National-Parks-Map-and-Image-Gallery.html</guid>
        
        
        <category>leaflet</category>
        
        <category>qgis</category>
        
        <category>turfjs</category>
        
        <category>lightbox</category>
        
        <category>flickr_api</category>
        
        <category>data.gov</category>
        
      </item>
    
      <item>
        <title>Introduction to Interactive Web Maps Using JavaScript and LeafletJS</title>
        <description>&lt;p&gt;This is an excellent beginning web application for anyone new to JavaScript and/or &lt;a href=&quot;http://leafletjs.com/&quot;&gt;LeafletJS&lt;/a&gt;.  The tutorial was developed by MIT and is easy to understand.  As someone who is grateful to all professors/peers who have shared their knowledge and experience with me over the years, I like to contribute when able.  I have included some modifications for aspiring web developers below that address publishing your application via &lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; as well as &lt;a href=&quot;http://bower.io/&quot;&gt;Bower&lt;/a&gt;, a package manager that makes including frameworks, libraries, and utilities in your projects more convenient for developers.  Altogether, this makes for an excellent start to learning about general web development as well as creating interactive web maps.&lt;/p&gt;

&lt;p&gt;The full MIT tutorial can be found &lt;a href=&quot;http://duspviz.mit.edu/leaflet-js/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;View my application &lt;a href=&quot;http://tannerkj.github.io/MIT-campus-coffee/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h6 id=&quot;adjustments-i-made-when-running-through-this-tutorial&quot;&gt;Adjustments I made when running through this tutorial:&lt;/h6&gt;

&lt;h6 id=&quot;step-1b&quot;&gt;Step 1.b&lt;/h6&gt;

&lt;p&gt;I did not serve locally using Python.  I created a &lt;a href=&quot;https://github.com/tannerkj/MIT-campus-coffee&quot;&gt;GitHub repo&lt;/a&gt; and viewed changes locally using &lt;em&gt;http://localhost/~username/projectname/&lt;/em&gt; before pushing to GitHub.  It is a good standard practice to follow.  Also, you then have the ability to publish your webpages using GitHub Pages.&lt;/p&gt;

&lt;h6 id=&quot;step-1d&quot;&gt;Step 1.d&lt;/h6&gt;

&lt;p&gt;I used &lt;a href=&quot;http://bower.io/&quot;&gt;Bower&lt;/a&gt;, as mentioned above, to add Leaflet to my project.  It would be prudent to become familiar with Bower and its dependencies since many common packages can be easily installed with its use.  Dependencies for Bower include &lt;a href=&quot;https://nodejs.org/&quot;&gt;Node and npm&lt;/a&gt; and &lt;a href=&quot;http://git-scm.com/&quot;&gt;Git&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Using command line, inside my working directory, I entered the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bower install leaflet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will notice a new folder in your working directory called &lt;em&gt;bower_components&lt;/em&gt; that contains the necessary Leaflet files for your project.  Therefore, the Leaflet CSS link in the &lt;em&gt;head&lt;/em&gt; section of your index.html would be referenced as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;link rel=&quot;stylesheet&quot; href=&quot;./bower_components/leaflet/dist/leaflet.css&quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the Leaflet JavaScript link at the bottom of the &lt;em&gt;body&lt;/em&gt; section would be referenced as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;script src=&quot;./bower_components/leaflet/dist/leaflet.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;View my application &lt;a href=&quot;http://tannerkj.github.io/MIT-campus-coffee/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;View my source code &lt;a href=&quot;https://github.com/tannerkj/MIT-campus-coffee&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/MIT_campus_coffee.png&quot; alt=&quot;MIT-campus-coffee&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 09 Apr 2015 00:00:00 -0700</pubDate>
        <link>http://tannergeo.com/blog/site//leaflet/github/bower/development/beginner/2015/04/09/Introduction-To-Interactive-Web-Maps-Using-JavaScript-and-LeafletJS.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//leaflet/github/bower/development/beginner/2015/04/09/Introduction-To-Interactive-Web-Maps-Using-JavaScript-and-LeafletJS.html</guid>
        
        
        <category>leaflet</category>
        
        <category>github</category>
        
        <category>bower</category>
        
        <category>development</category>
        
        <category>beginner</category>
        
      </item>
    
      <item>
        <title>Visualizing Percentage of Storms Resulting in Injuries/Deaths by State (1996 - 2014)</title>
        <description>&lt;p&gt;Starting in 1996, &lt;a href=&quot;http://www.ncdc.noaa.gov/stormevents/pd01016005curr.pdf&quot;&gt;National Weather Service directive 10-1605&lt;/a&gt; mandated the reporting of 48 different types of weather events and their effects on the impacted community.  This tutorial aims to create a visualization focusing on the percent of storms resulting in injuries/deaths reported by state, and to identify which states 
have been more fortunate than others.  The data is provided by &lt;a href=&quot;http://www.ncdc.noaa.gov/stormevents/ftp.jsp&quot;&gt;NOAA&lt;/a&gt; in .csv files organized by individual year.  This tutorial requires a working knowledge of the command line, &lt;a href=&quot;http://postgis.net/&quot;&gt;PostgreSQL/PostGIS&lt;/a&gt;, &lt;a href=&quot;http://www.gdal.org/&quot;&gt;GDAL/ogr2ogr&lt;/a&gt;, &lt;a href=&quot;http://leafletjs.com/&quot;&gt;LeafletJS&lt;/a&gt;, Python, and JavaScript.&lt;/p&gt;

&lt;p&gt;The first step is to create the tables in PostgreSQL and load the data.  A new database is created and the following code executed to enable PostGIS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE EXTENSION postgis;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, the code to generate the table is executed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TABLE storm_events (
	state varchar(20),
	year smallint,
	state_fips smallint,	
	event_type varchar(50),
	injuries_direct smallint,
	injuries_indirect smallint,
	deaths_direct smallint,
	deaths_indirect smallint
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Included are the attributes we need for this application, along with additional fields we may opt to use in the future for further analysis.&lt;/p&gt;

&lt;p&gt;To create a surrogate primary key called id for this table, the following code is executed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALTER TABLE storm_events ADD COLUMN id SERIAL PRIMARY KEY;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To load the data into the table, several options are available.  One option, though not the most efficient, is outlined below:&lt;/p&gt;

&lt;p&gt;1) A python script is written to read from each year’s .csv file in our details_raw folder and creates newly formatted .csv files, excluding the unnecessary data.  It is then executed in the appropriate directory using the command line.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# data_loader.py
# reads .csv files in ./details_raw
# creates new .csv files in a separate directory

import os, csv

in_dir = &quot;./details_raw&quot;

def filter_fields(f):
	with open(root + &quot;/&quot; + f, &quot;rb&quot;) as source:
    	reader = csv.DictReader(source)
    	outf = root + &quot;/formatted/&quot; + f
    	with open(outf, &quot;wb&quot;) as result:
      		writer = csv.DictWriter(result, fieldnames = [&#39;STATE&#39;, &#39;YEAR&#39;,&#39;STATE_FIPS&#39;, 
      		&#39;EVENT_TYPE&#39;, &#39;INJURIES_DIRECT&#39;, &#39;INJURIES_INDIRECT&#39;, &#39;DEATHS_DIRECT&#39;, &#39;DEATHS_INDIRECT&#39;])
        	print (&quot;Writing&quot;, outf)
        	writer.writeheader()
        	for row in reader:
            	writer.writerow({&#39;STATE&#39; : row[&#39;STATE&#39;],
                             	&#39;YEAR&#39; : row[&#39;YEAR&#39;],
                             	&#39;STATE_FIPS&#39; : row[&#39;STATE_FIPS&#39;],
                             	&#39;EVENT_TYPE&#39; : row[&#39;EVENT_TYPE&#39;],
                             	&#39;INJURIES_DIRECT&#39; : row[&#39;INJURIES_DIRECT&#39;],
                             	&#39;INJURIES_INDIRECT&#39; : row[&#39;INJURIES_INDIRECT&#39;],
                             	&#39;DEATHS_DIRECT&#39; : row[&#39;DEATHS_DIRECT&#39;],
                             	&#39;DEATHS_INDIRECT&#39; : row[&#39;DEATHS_INDIRECT&#39;]})

for root, dirs, files in os.walk(in_dir):
	for f in files:
    	filter_fields(f)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2) Using postgres command line tools for each csv file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;psql -U postgres

\connect geodata
\copy storm_events from &#39;/Path to csv&#39; delimiter &#39;,&#39; csv header
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, to create views that allow us to analyze the data you have several options but below are a few simple ones that isolate the info we are interested in.  They also allow room for growth should you choose to add aggregates of other attributes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW injuries_deaths AS
SELECT storm_events.state, storm_events.state_fips,
count(*) AS count
FROM storm_events
WHERE storm_events.injuries_direct &amp;lt;&amp;gt; 0 OR
storm_events.injuries_indirect 	&amp;lt;&amp;gt; 0 OR
storm_events.deaths_direct &amp;lt;&amp;gt; 0 OR
storm_events.deaths_indirect &amp;lt;&amp;gt; 0
GROUP BY storm_events.state, state_fips
ORDER BY storm_events.state;

CREATE VIEW state_totals AS
SELECT total.state, total.state_fips,
count(*) AS count
FROM (SELECT storm_events.state,
       storm_events.year,
       storm_events.state_fips,
       storm_events.event_type,
       storm_events.injuries_direct,
       storm_events.injuries_indirect,
       storm_events.deaths_direct,
       storm_events.deaths_indirect
      FROM storm_events) total
GROUP BY total.state, total.state_fips
ORDER BY total.state;

CREATE VIEW map_storm_data AS
(SELECT s.state, s.state_fips, id.count injuries_death_count, s.count 
total_storms_count, 
round(((id.count::float/s.count::float)*100)::numeric,2) percent
FROM injuries_deaths id
LEFT JOIN state_totals s ON id.state = s.state);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A simple query to validate the data and identify the states with the highest percentage of storms resulting in injury/death:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT *
FROM map_storm_data
ORDER BY percent DESC;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To generate a shapefile we can use to visualize our data,  download from TIGER the shapefile needed, and then use command line to convert it to the desired projection and import it into postgres.  In this case, I entered:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shp2pgsql -s 4269:4326 tl_2014_us_state  public.states | psql -h 
localhost -d geodata -U postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now to create a single view that includes the desired calculations along with the associated state geometry data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW state_storm_percent AS
SELECT a.name, b.state_fips, b.injuries_death_count,
b.total_storms_count, b.percent, ST_Simplify(a.geom, 0.005) as geom
FROM states a
LEFT JOIN map_storm_data b
ON a.statefp::int = b.state_fips;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For constructing the application, we will use Leaflet and GeoJSON.  To do this, we will export the map as a shapefile, and use &lt;a href=&quot;http://www.gdal.org/&quot;&gt;GDAL&lt;/a&gt; to convert it to GeoJSON.&lt;/p&gt;

&lt;p&gt;The following code is executed in the command line for PostgreSQL to generate the shapefile and then convert to geojson:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pgsql2shp -f storm_events -h localhost -p 5432 -u postgres -P password
geodata public.state_storm_percent;

ogr2ogr -f GeoJSON storm_events.geojson storm_events.shp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create the map for our application, we will use the same process as described in this &lt;a href=&quot;http://leafletjs.com/examples/choropleth.html&quot;&gt;Leaflet tutorial&lt;/a&gt;.  The end result is a thematic map with a legend and an info window describing the data as you select each state.&lt;/p&gt;

&lt;p&gt;View application &lt;a href=&quot;http://tannerkj.github.io/NOAA_Storm_Events/index.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Thu, 19 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://tannergeo.com/blog/site//postgis/leaflet/noaa/postgresql/2015/03/19/Visualizing-Percentage-of-Storms-Resulting-in-Injury-or-Death-by-State.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//postgis/leaflet/noaa/postgresql/2015/03/19/Visualizing-Percentage-of-Storms-Resulting-in-Injury-or-Death-by-State.html</guid>
        
        
        <category>postgis</category>
        
        <category>leaflet</category>
        
        <category>noaa</category>
        
        <category>postgresql</category>
        
      </item>
    
      <item>
        <title>A Brief Look At Vector Tiles</title>
        <description>&lt;p&gt;This post is a very ‘basic’ look into vector tiles.  Feel free to make improvements to this post through &lt;a href=&quot;https://github.com/tannerjt/blog_posts/blob/master/vector_tiles.md&quot;&gt;github&lt;/a&gt;.  Also, please use the links throughout this document to understand more about vector tiles.&lt;/p&gt;

&lt;p&gt;At ESRI’s 2015 Developer Conference, the plan to &lt;a href=&quot;http://video.esri.com/watch/4215/smart-mapping-with-vector-map-tiles&quot;&gt;implement vector tiles&lt;/a&gt; into their mapping platform was announced.  Most traditional GIS analysts and developers have created &lt;a href=&quot;http://resources.arcgis.com/en/help/main/10.2/index.html#//001700000189000000&quot;&gt;tile caches&lt;/a&gt; or &lt;a href=&quot;https://developers.arcgis.com/javascript/jsapi/arcgistiledmapservicelayer-amd.html&quot;&gt;consumed tiled map service layers&lt;/a&gt; to implement complex geometries with &lt;a href=&quot;http://www.usgs.gov/faq/categories/9860/3604&quot;&gt;more efficiency&lt;/a&gt; as part of their business processes.  To most of the industry (I’m assuming), &lt;a href=&quot;http://wiki.openstreetmap.org/wiki/Vector_tiles&quot;&gt;vector tiles&lt;/a&gt; is a new concept that offers many advantages over traditional formats.  Although the concept is similar to raster tiles, vector tiles return the actual features which creates the ability to manipulate the style and symbology of features dynamically.  This can be done, all while benefiting from the efficiency of only needing to load tiles within the current extent and not the entirety of a features geometry and attributes.  Also, vector tiles also benefit over feature services because the geometry is essentially cached and not needed to be dynamically created on the server in most cases.&lt;/p&gt;

&lt;p&gt;It is important to know that although vector tiles are new to Esri’s platform, they are already present in the geospatial community elsewhere.  &lt;a href=&quot;https://www.mapbox.com/&quot;&gt;Mapbox&lt;/a&gt;  has &lt;a href=&quot;https://www.mapbox.com/blog/vector-tiles/&quot;&gt;developed a standard for an open source vector tile format&lt;/a&gt; and uses it at the core of &lt;a href=&quot;https://www.mapbox.com/tilemill/&quot;&gt;tilemill&lt;/a&gt;.  In fact, &lt;a href=&quot;https://www.mapbox.com/blog/vector-tile-adoption/&quot;&gt;Esri has adopted&lt;/a&gt; this specification in it’s implementation of vector tiles.  If you look at the vector tile spec defined by mapbox, you will also see various &lt;a href=&quot;https://github.com/mapbox/vector-tile-spec/wiki/Implementations&quot;&gt;applications and tools that implement it&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Within &lt;a href=&quot;https://developers.arcgis.com/javascript/jsapi/&quot;&gt;Esri’s current JavaScript API&lt;/a&gt;, I don’t see support for vector tiles but I’m sure the next release will include it.  OpenLayers  does support &lt;a href=&quot;http://openlayers.org/en/v3.3.0/apidoc/ol.source.TileVector.html&quot;&gt;a TileVector&lt;/a&gt; format, as illustrated in &lt;a href=&quot;http://openlayers.org/en/v3.3.0/examples/tile-vector.html&quot;&gt;this example&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are various tile-generation libraries available for creating vector tiles, and according to Esri’s announcement, a toolbox will be available directly though their Desktop (or Pro) software.  One nice example I found was created by &lt;a href=&quot;https://github.com/glob3mobile&quot;&gt;Glob3 Mobile&lt;/a&gt; called &lt;a href=&quot;https://github.com/glob3mobile/mmt-vector-tiles&quot;&gt;mmt-vector-tiles&lt;/a&gt; which can generate vector tiles directly from a PostGIS database.  They also have a &lt;a href=&quot;http://wb.glob3mobile.com/vl/index_lux.html&quot;&gt;good example&lt;/a&gt; posted of this in action with OpenLayers 3.  If you look at the &lt;a href=&quot;http://igosoftware.dyndns.org:8000/vectorial/lux_buildings_LEVELS_12-18_MERCATOR/GEOJSON/&quot;&gt;file stucture&lt;/a&gt; of their tiles, it looks similar to what is shown in &lt;a href=&quot;http://video.esri.com/watch/4215/smart-mapping-with-vector-map-tiles&quot;&gt;Esri’s presentation&lt;/a&gt; although I’m not certain it adheres to the same specification.&lt;/p&gt;

&lt;p&gt;If you haven’t heard of vector tiles before, I hope this post has been useful!&lt;/p&gt;

</description>
        <pubDate>Sun, 15 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://tannergeo.com/blog/site//vector_tiles/formats/2015/03/15/A-Brief-Look-At-Vector-Tiles.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//vector_tiles/formats/2015/03/15/A-Brief-Look-At-Vector-Tiles.html</guid>
        
        
        <category>vector_tiles</category>
        
        <category>formats</category>
        
      </item>
    
      <item>
        <title>Visualizing February 2015 Weather Data with Turf.js</title>
        <description>&lt;p&gt;February was a particularly cold month across most of the United States.  For us here in Oregon, we’ve been enjoying warm weather… but most further east have had record lows and heavy snow.  I’ve been interested in finding new ways to visualize this data.  I’ll be using &lt;a href=&quot;http://cdo.ncdc.noaa.gov/qclcd/QCLCD?prior=N&quot;&gt;NOAA’s quality controlled logical climatological data&lt;/a&gt; to load weather station data across the US into a Postgres database.  From there I hope to generate TIN’s using &lt;a href=&quot;http://turfjs.org/&quot;&gt;turf.js&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The initial hurdle is getting the stations and readings into our database.  Our file from NOAA has the following pipe delineated headers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WBAN|WMO|CallSign|ClimateDivisionCode|ClimateDivisionStateCode|ClimateDivisionStationCode|Name|State|Location|Latitude|Longitude|GroundHeight|StationHeight|Barometer|TimeZone
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I don’t need all of these, just the WBAN (unique id of station), State, Location, Latitude and Longitude.  Let’s create the initial table:&lt;/p&gt;

&lt;p&gt;Enable postgis on database:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE EXTENSION postgis;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create table for stations:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TABLE weather_stations (
    WBAN integer,
    State char(2),
    Location varchar(150),
    Latitude decimal(9,6) NOT NULL,
    Longitude decimal(9,6) NOT NULL
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I deleted the erroneous columns from the original file for easy import.  From psql command line, I ran the &lt;a href=&quot;http://stackoverflow.com/questions/2987433/how-to-import-csv-file-data-into-a-postgres-table&quot;&gt;following command&lt;/a&gt; to bring the stations into postgres:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;\copy weather_stations FROM &#39;csv location&#39; DELIMITER &#39;|&#39; CSV HEADER;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add an auto incremented primary key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALTER TABLE weather_stations ADD COLUMN id SERIAL PRIMARY KEY;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;http://postgis.net/docs/AddGeometryColumn.html&quot;&gt;Add geometry column&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT AddGeometryColumn (&#39;public&#39;, &#39;weather_stations&#39;, &#39;geom&#39;, 4326, &#39;POINT&#39;, 2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, let’s create actual points from our Latitude and Longitude fields:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;UPDATE weather_stations SET
geom = ST_SetSRID(ST_MakePoint(Longitude, Latitude),4326)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we look at our data in QGIS, we see this now!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/points_qgis.png&quot; alt=&quot;qgis screenshpt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that the stations are in our database, we need to import our weather station readings.  The process will be similar to what we accomplished getting the stations in.  The readings are comma delineated and contain information or daily recordings.  The initial format looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WBAN,YearMonthDay,Tmax,TmaxFlag,Tmin,TminFlag,Tavg,TavgFlag,Depart,DepartFlag,DewPoint,DewPointFlag,WetBulb,WetBulbFlag,Heat,HeatFlag,Cool,CoolFlag,Sunrise,SunriseFlag,Sunset,SunsetFlag,CodeSum,CodeSumFlag,Depth,DepthFlag,Water1,Water1Flag,SnowFall,SnowFallFlag,PrecipTotal,PrecipTotalFlag,StnPressure,StnPressureFlag,SeaLevel,SeaLevelFlag,ResultSpeed,ResultSpeedFlag,ResultDir,ResultDirFlag,AvgSpeed,AvgSpeedFlag,Max5Speed,Max5SpeedFlag,Max5Dir,Max5DirFlag,Max2Speed,Max2SpeedFlag,Max2Dir,Max2DirFlag
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously, we are only interested in the WBAN (station id), YearMonthDay, Tmax, Tmin and Tavg.  These will give us the ability to map low, high and average daily temperatures.  I went ahead and removed the other columns from the initial dataset.  Let’s create the table in postgres.  I used varchar for the temperatures because of some string M values which equal missing… we can filter these out later.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TABLE readings_feb (
    WBAN integer,
    YearMonthDay integer NOT NULL,
    Tmax varchar(5),
    Tmin varchar(5),
    Tavg varchar(5)
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Import csv file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;\copy readings_feb FROM &#39;D:\TannerGeo\WeatherTinTurf\data_raw\QCLCD201502\201502daily.csv&#39; DELIMITER &#39;,&#39; CSV HEADER;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, add a primary key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALTER TABLE readings_feb ADD COLUMN id SERIAL PRIMARY KEY;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To visualize the data in QGIS, I created a special view with just the low temp.  Notice we’ve casted the &lt;a href=&quot;http://dba.stackexchange.com/questions/3429/how-can-i-convert-from-double-precision-to-bigint-with-postgresql&quot;&gt;tmin to int&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW feb_weather AS
SELECT a.id, a.wban, a.yearmonthday, a.tmin::int, b.state, b.location, b.geom
FROM readings_feb a
LEFT JOIN weather_stations b
ON a.wban = b.wban
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By adding a filter for just February 1st in QGIS and symbolizing by temperature, the cold weather trends become apparent.  The mid-west and northeast have bone chilling lows while the west coast and southern US had relatively high temperatures.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/temp_feb01.png&quot; alt=&quot;February Map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To incorporate the data into our mapping application, it’s preferable that we work with a data format suited to our needs.  I’ll be using &lt;a href=&quot;http://geojson.org/&quot;&gt;geojson&lt;/a&gt;, mostly because this format will work best with &lt;a href=&quot;http://turfjs.org/static/docs/global.html#GeoJSON&quot;&gt;turf.js&lt;/a&gt;.  We could &lt;a href=&quot;http://www.postgresonline.com/journal/archives/267-Creating-GeoJSON-Feature-Collections-with-JSON-and-PostGIS-functions.html&quot;&gt;build a query&lt;/a&gt; to generate this straight from postgres, but I’d prefer to just use &lt;a href=&quot;http://www.bostongis.com/pgsql2shp_shp2pgsql_quickguide.bqg&quot;&gt;pgsql2shp&lt;/a&gt; and then &lt;a href=&quot;http://www.gdal.org/ogr2ogr.html&quot;&gt;GDAL’s ogr2ogr&lt;/a&gt; to convert to geojson.&lt;/p&gt;

&lt;p&gt;Create shapefile using pgsql command line tools:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pgsql2shp -f weather_02012015 -h localhost -p 5432 -u username -P password geodata &quot;SELECT * FROM feb_weather WHERE yearmonthday = 20150201&quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Convert to GeoJSON using &lt;a href=&quot;http://www.gisinternals.com/query.html?content=filelist&amp;amp;file=release-1800-x64-gdal-1-11-1-mapserver-6-4-1.zip&quot;&gt;GDAL&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ogr2ogr -f GeoJSON ../geojson/02012015.geojson 02012015.shp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our output file is 287KB could probably be smaller since we don’t necessarily need all of the fields to make the TIN map.&lt;/p&gt;

&lt;p&gt;The only dependencies I know of that we need to create the map are turf.js and leaflet.  I chose to use bower to install these dependencies:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bower install turf
bower install leaflet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For bringing in the geojson, I’m just going to assign the object to a variable and place it in a seperate .js file.   For future reference though, &lt;a href=&quot;https://github.com/calvinmetcalf&quot;&gt;calvinmetcalf&lt;/a&gt; has created a great helper for leaflet called the &lt;a href=&quot;https://github.com/calvinmetcalf/leaflet-ajax&quot;&gt;leaflet-ajax&lt;/a&gt; library for working with geojson files.&lt;/p&gt;

&lt;p&gt;We now have a .js file called 02012015.js with the geojson assigned to the variable feb01.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var feb01 = {
    type&quot;: &quot;FeatureCollection&quot;,
	&quot;crs&quot;: { &quot;type&quot;: &quot;name&quot;, &quot;properties&quot;: { &quot;name&quot;: &quot;urn:ogc:def:crs:OGC:1.3:CRS84&quot; } },
	...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I want to first make sure I can create the TIN’s.  To test, I use turf.js to build and add them to the map with no fill.  By passing in TMIN, turf generates Z values that we can then use to generate our fill colors.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var tin = turf.tin(feb01, &#39;TMIN&#39;);
var geojson = L.geoJson(tin, {
	style : {
		weight : 1
	}
});
geojson.addTo(map);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our result is a visually interesting graphic:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/tin_nofill.png&quot; alt=&quot;tin image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For our fill colors, I’m going to use a library I built called &lt;a href=&quot;https://github.com/tannerjt/classybrew&quot;&gt;classybrew&lt;/a&gt;.  It uses the natural breaks (Jenks) method of statistical classification and applies colors based on Cynthia Brewers color palettes.  I added a new diverging color palette for red-yellow-blue.  This should give us a good thematic perspective of color.&lt;/p&gt;

&lt;p&gt;To stylize the map, we first need to initialize and setup or statistical breaks and color palette based on our minimum temperature values.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// create classification for colors
var brew = new classyBrew();
var values = []; // tmin values
for( var i = 0; i &amp;lt; feb01.features.length; i++) {
	values.push(feb01.features[i].properties[&#39;TMIN&#39;]);
}
brew.setSeries(values);
brew.setNumClasses(6);
brew.classify();
brew.setColorCode(&quot;BuYlRd&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have the classybrew object setup, we can request a color code based on a provided value with &lt;code&gt;getColorInRange(value)&lt;/code&gt;.  We’ll use this function to return a color based on the average of minimum temperature from each station that makes up our triangles.  This can be done during instantiation of our geojson layer.  First, however we need to create our TIN from our points using &lt;code&gt;turf.tin(geojson, z value)&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var tin = turf.tin(feb01, &#39;TMIN&#39;);
var geojson = L.geoJson(tin, {
	style : function (f) {
		return {
			weight : 1,
			fillColor : (function () {
				return brew.getColorInRange((f.properties.a + f.properties.b + f.properties.c) / 3.0, true);
			}()),
			fillOpacity : 0.85
		}
	}
});
geojson.addTo(map);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our result is rather captivating:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/tin_fill.png&quot; alt=&quot;Weather TIN Map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we look at a map from the NOAA National Climatic Data Center, we can see our map aligns with their model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/noaa_temp.png&quot; alt=&quot;NOAA NCDC Map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I think this is all I was looking to do for now.  The output is visually interesting and shows how powerful turf.js is with very little code.  I’d like to take this further and make a more in depth application with the data and turf.js.  Look for more to come in the future, and thanks for following along!&lt;/p&gt;
</description>
<<<<<<< HEAD
        <pubDate>Wed, 03 Dec 2014 10:44:02 -0800</pubDate>
<<<<<<< HEAD
        <link>http://tannergeo.com/~joshuatanner/tannergeo//jekyll/update/2014/12/03/welcome-to-jekyll.html</link>
        <guid isPermaLink="true">http://tannergeo.com/~joshuatanner/tannergeo//jekyll/update/2014/12/03/welcome-to-jekyll.html</guid>
=======
        <link>http://tannergeo.com/blog/site//jekyll/update/2014/12/03/welcome-to-jekyll.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//jekyll/update/2014/12/03/welcome-to-jekyll.html</guid>
>>>>>>> gh-pages
=======
        <pubDate>Sun, 01 Mar 2015 00:00:00 -0800</pubDate>
        <link>http://tannergeo.com/blog/site//turfjs/leaflet/postgresql/postgis/noaa/2015/03/01/Visualizing-February-2015-Weather-Date-with-Turf.js.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//turfjs/leaflet/postgresql/postgis/noaa/2015/03/01/Visualizing-February-2015-Weather-Date-with-Turf.js.html</guid>
        
        
        <category>turfjs</category>
>>>>>>> gh-pages
        
        <category>leaflet</category>
        
        <category>postgresql</category>
        
        <category>postgis</category>
        
        <category>noaa</category>
        
      </item>
    
  </channel>
</rss>
