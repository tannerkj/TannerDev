<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TannerGeo Blog</title>
    <description>TannerGeo is passionate about computer science, web development and everything geospatial!  We appreciate the helpful tutorials and blog posts others have created.  We would like to give back as well by posting our  experiences and tutorials on new technologies.
</description>
    <link>http://tannergeo.com/blog/site//</link>
    <atom:link href="http://tannergeo.com/blog/site//feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 13 Apr 2015 09:08:35 -0700</pubDate>
    <lastBuildDate>Mon, 13 Apr 2015 09:08:35 -0700</lastBuildDate>
    <generator>Jekyll v2.5.2</generator>
    
      <item>
        <title>Introduction to Interactive Web Maps Using JavaScript and LeafletJS</title>
        <description>&lt;p&gt;This is an excellent beginning web application for anyone new to JavaScript and/or &lt;a href=&quot;http://leafletjs.com/&quot;&gt;LeafletJS&lt;/a&gt;.  The tutorial was developed by MIT and is easy to understand.  As someone who is grateful to all professors/peers who have shared their knowledge and experience with me over the years, I like to contribute when able.  I have included some modifications for aspiring web developers below that address publishing your application via &lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; as well as &lt;a href=&quot;http://bower.io/&quot;&gt;Bower&lt;/a&gt;, a package manager that makes including frameworks, libraries, and utilities in your projects more convenient for developers.  Altogether, this makes for an excellent start to learning about general web development as well as creating interactive web maps.&lt;/p&gt;

&lt;p&gt;The full MIT tutorial can be found &lt;a href=&quot;http://duspviz.mit.edu/leaflet-js/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;View my application &lt;a href=&quot;http://tannerkj.github.io/MIT-campus-coffee/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h6 id=&quot;adjustments-i-made-when-running-through-this-tutorial&quot;&gt;Adjustments I made when running through this tutorial:&lt;/h6&gt;

&lt;h6 id=&quot;step-1b&quot;&gt;Step 1.b&lt;/h6&gt;

&lt;p&gt;I did not serve locally using Python.  I created a &lt;a href=&quot;https://github.com/tannerkj/MIT-campus-coffee&quot;&gt;GitHub repo&lt;/a&gt; and viewed changes locally using &lt;em&gt;http://localhost/~username/projectname/&lt;/em&gt; before pushing to GitHub.  It is a good standard practice to follow.  Also, you then have the ability to publish your webpages using GitHub Pages.  &lt;/p&gt;

&lt;h6 id=&quot;step-1d&quot;&gt;Step 1.d&lt;/h6&gt;

&lt;p&gt;I used &lt;a href=&quot;http://bower.io/&quot;&gt;Bower&lt;/a&gt;, as mentioned above, to add Leaflet to my project.  It would be prudent to become familiar with Bower and its dependencies since many common packages can be easily installed with its use.  Dependencies for Bower include &lt;a href=&quot;https://nodejs.org/&quot;&gt;Node and npm&lt;/a&gt; and &lt;a href=&quot;http://git-scm.com/&quot;&gt;Git&lt;/a&gt;.  &lt;/p&gt;

&lt;p&gt;Using command line, inside my working directory, I entered the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bower install leaflet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will notice a new folder in your working directory called &lt;em&gt;bower_components&lt;/em&gt; that contains the necessary Leaflet files for your project.  Therefore, the Leaflet CSS link in the &lt;em&gt;head&lt;/em&gt; section of your index.html would be referenced as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;link rel=&quot;stylesheet&quot; href=&quot;./bower_components/leaflet/dist/leaflet.css&quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the Leaflet JavaScript link at the bottom of the &lt;em&gt;body&lt;/em&gt; section would be referenced as: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;script src=&quot;./bower_components/leaflet/dist/leaflet.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;View my application &lt;a href=&quot;http://tannerkj.github.io/MIT-campus-coffee/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;View my source code &lt;a href=&quot;https://github.com/tannerkj/MIT-campus-coffee&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/MIT_campus_coffee.png&quot; alt=&quot;MIT-campus-coffee&quot; /&gt;		&lt;/p&gt;

</description>
        <pubDate>Thu, 09 Apr 2015 00:00:00 -0700</pubDate>
        <link>http://tannergeo.com/blog/site//leaflet/github/bower/development/beginner/2015/04/09/Introduction-To-Interactive-Web-Maps-Using-JavaScript-and-LeafletJS.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//leaflet/github/bower/development/beginner/2015/04/09/Introduction-To-Interactive-Web-Maps-Using-JavaScript-and-LeafletJS.html</guid>
        
        
        <category>leaflet</category>
        
        <category>github</category>
        
        <category>bower</category>
        
        <category>development</category>
        
        <category>beginner</category>
        
      </item>
    
      <item>
        <title>Visualizing Percentage of Storms Resulting in Injuries/Deaths by State (1996 - 2014)</title>
        <description>&lt;p&gt;Starting in 1996, &lt;a href=&quot;http://www.ncdc.noaa.gov/stormevents/pd01016005curr.pdf&quot;&gt;National Weather Service directive 10-1605&lt;/a&gt; mandated the reporting of 48 different types of weather events and their effects on the impacted community.  This tutorial aims to create a visualization focusing on the percent of storms resulting in injuries/deaths reported by state, and to identify which states 
have been more fortunate than others.  The data is provided by &lt;a href=&quot;http://www.ncdc.noaa.gov/stormevents/ftp.jsp&quot;&gt;NOAA&lt;/a&gt; in .csv files organized by individual year.&lt;/p&gt;

&lt;p&gt;The first step is to create the tables in PostgreSQL and load the data.  A new database is created and the following code executed to enable postgis:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE EXTENSION postgis;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, the code to generate the table is executed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TABLE storm_events (
	state varchar(20),
	year smallint,
	state_fips smallint,	
	event_type varchar(50),
	injuries_direct smallint,
	injuries_indirect smallint,
	deaths_direct smallint,
	deaths_indirect smallint
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Included are the attributes we need for this application, along with additional fields we may opt to use in the future for further analysis.&lt;/p&gt;

&lt;p&gt;To create a surrogate primary key called id for this table, the following code is executed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALTER TABLE storm_events ADD COLUMN id SERIAL PRIMARY KEY;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To load the data into the table, several options are available.  One option, though not the most efficient, is outlined below:&lt;/p&gt;

&lt;p&gt;1) A python script is written to read from each year’s .csv file in our details_raw folder and creates newly formatted .csv files, excluding the unnecessary data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# data_loader.py
# reads .csv files in ./details_raw
# creates new .csv files in a separate directory

import os, csv

in_dir = &quot;./details_raw&quot;

def filter_fields(f):
	with open(root + &quot;/&quot; + f, &quot;rb&quot;) as source:
    	reader = csv.DictReader(source)
    	outf = root + &quot;/formatted/&quot; + f
    	with open(outf, &quot;wb&quot;) as result:
      		writer = csv.DictWriter(result, fieldnames = [&#39;STATE&#39;, &#39;YEAR&#39;,&#39;STATE_FIPS&#39;, 
      		&#39;EVENT_TYPE&#39;, &#39;INJURIES_DIRECT&#39;, &#39;INJURIES_INDIRECT&#39;, &#39;DEATHS_DIRECT&#39;, &#39;DEATHS_INDIRECT&#39;])
        	print (&quot;Writing&quot;, outf)
        	writer.writeheader()
        	for row in reader:
            	writer.writerow({&#39;STATE&#39; : row[&#39;STATE&#39;],
                             	&#39;YEAR&#39; : row[&#39;YEAR&#39;],
                             	&#39;STATE_FIPS&#39; : row[&#39;STATE_FIPS&#39;],
                             	&#39;EVENT_TYPE&#39; : row[&#39;EVENT_TYPE&#39;],
                             	&#39;INJURIES_DIRECT&#39; : row[&#39;INJURIES_DIRECT&#39;],
                             	&#39;INJURIES_INDIRECT&#39; : row[&#39;INJURIES_INDIRECT&#39;],
                             	&#39;DEATHS_DIRECT&#39; : row[&#39;DEATHS_DIRECT&#39;],
                             	&#39;DEATHS_INDIRECT&#39; : row[&#39;DEATHS_INDIRECT&#39;]})

for root, dirs, files in os.walk(in_dir):
	for f in files:
    	filter_fields(f)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2) Using postgres command line tools for each csv file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;psql -U postgres

\connect geodata
\copy storm_events from &#39;/Path to csv&#39; delimiter &#39;,&#39; csv header
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, to create views that allow us to analyze the data you have several options but below are a few simple ones that isolate the info we are interested in.  They also allow room for growth should you choose to add aggregates of other attributes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW injuries_deaths AS
SELECT storm_events.state, storm_events.state_fips,
count(*) AS count
FROM storm_events
WHERE storm_events.injuries_direct &amp;lt;&amp;gt; 0 OR
storm_events.injuries_indirect 	&amp;lt;&amp;gt; 0 OR
storm_events.deaths_direct &amp;lt;&amp;gt; 0 OR
storm_events.deaths_indirect &amp;lt;&amp;gt; 0
GROUP BY storm_events.state, state_fips
ORDER BY storm_events.state;

CREATE VIEW state_totals AS
SELECT total.state, total.state_fips,
count(*) AS count
FROM (SELECT storm_events.state,
       storm_events.year,
       storm_events.state_fips,
       storm_events.event_type,
       storm_events.injuries_direct,
       storm_events.injuries_indirect,
       storm_events.deaths_direct,
       storm_events.deaths_indirect
      FROM storm_events) total
GROUP BY total.state, total.state_fips
ORDER BY total.state;

CREATE VIEW map_storm_data AS
(SELECT s.state, s.state_fips, id.count injuries_death_count, s.count 
total_storms_count, 
round(((id.count::float/s.count::float)*100)::numeric,2) percent
FROM injuries_deaths id
LEFT JOIN state_totals s ON id.state = s.state);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A simple query to validate the data and identify the states with the highest percentage of storms resulting in injury/death:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT *
FROM map_storm_data
ORDER BY percent DESC;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To generate a shapefile we can use to visualize our data,  download from TIGER the shapefile needed, and then use command line to convert it to the desired projection and import it into postgres.  In this case, I entered:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shp2pgsql -s 4269:4326 tl_2014_us_state  public.states | psql -h 
localhost -d geodata -U postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now to create a single view that includes the desired calculations along with the associated state geometry data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW state_storm_percent AS
SELECT a.name, b.state_fips, b.injuries_death_count,
b.total_storms_count, b.percent, ST_Simplify(a.geom, 0.005) as geom
FROM states a
LEFT JOIN map_storm_data b
ON a.statefp::int = b.state_fips;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For constructing the application, we will use Leaflet and GeoJSON.  To do this, we will export the map as a shapefile, and use gdal to convert it to GeoJSON.&lt;/p&gt;

&lt;p&gt;The following code is executed in the command line for PostgreSQL to generate the shapefile and then convert to geojson:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pgsql2shp -f storm_events -h localhost -p 5432 -u postgres -P password
geodata public.state_storm_percent;

ogr2ogr -f GeoJSON storm_events.geojson storm_events.shp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create the map for our application, we will use the same process as described in this &lt;a href=&quot;http://leafletjs.com/examples/choropleth.html&quot;&gt;Leaflet tutorial&lt;/a&gt;.  The end result is a thematic map with a legend and an info window describing the data as you select each state.&lt;/p&gt;

&lt;p&gt;View application &lt;a href=&quot;http://tannerkj.github.io/NOAA_Storm_Events/index.html&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;

</description>
        <pubDate>Thu, 19 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://tannergeo.com/blog/site//postgis/leaflet/noaa/postgresql/2015/03/19/Visualizing-Percentage-of-Storms-Resulting-in-Injury-or-Death-by-State.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//postgis/leaflet/noaa/postgresql/2015/03/19/Visualizing-Percentage-of-Storms-Resulting-in-Injury-or-Death-by-State.html</guid>
        
        
        <category>postgis</category>
        
        <category>leaflet</category>
        
        <category>noaa</category>
        
        <category>postgresql</category>
        
      </item>
    
      <item>
        <title>A Brief Look At Vector Tiles</title>
        <description>&lt;p&gt;This post is a very ‘basic’ look into vector tiles.  Feel free to make improvements to this post through &lt;a href=&quot;https://github.com/tannerjt/blog_posts/blob/master/vector_tiles.md&quot;&gt;github&lt;/a&gt;.  Also, please use the links throughout this document to understand more about vector tiles. &lt;/p&gt;

&lt;p&gt;At ESRI’s 2015 Developer Conference, the plan to &lt;a href=&quot;http://video.esri.com/watch/4215/smart-mapping-with-vector-map-tiles&quot;&gt;implement vector tiles&lt;/a&gt; into their mapping platform was announced.  Most traditional GIS analysts and developers have created &lt;a href=&quot;http://resources.arcgis.com/en/help/main/10.2/index.html#//001700000189000000&quot;&gt;tile caches&lt;/a&gt; or &lt;a href=&quot;https://developers.arcgis.com/javascript/jsapi/arcgistiledmapservicelayer-amd.html&quot;&gt;consumed tiled map service layers&lt;/a&gt; to implement complex geometries with &lt;a href=&quot;http://www.usgs.gov/faq/categories/9860/3604&quot;&gt;more efficiency&lt;/a&gt; as part of their business processes.  To most of the industry (I’m assuming), &lt;a href=&quot;http://wiki.openstreetmap.org/wiki/Vector_tiles&quot;&gt;vector tiles&lt;/a&gt; is a new concept that offers many advantages over traditional formats.  Although the concept is similar to raster tiles, vector tiles return the actual features which creates the ability to manipulate the style and symbology of features dynamically.  This can be done, all while benefiting from the efficiency of only needing to load tiles within the current extent and not the entirety of a features geometry and attributes.  Also, vector tiles also benefit over feature services because the geometry is essentially cached and not needed to be dynamically created on the server in most cases.&lt;/p&gt;

&lt;p&gt;It is important to know that although vector tiles are new to Esri’s platform, they are already present in the geospatial community elsewhere.  &lt;a href=&quot;https://www.mapbox.com/&quot;&gt;Mapbox&lt;/a&gt;  has &lt;a href=&quot;https://www.mapbox.com/blog/vector-tiles/&quot;&gt;developed a standard for an open source vector tile format&lt;/a&gt; and uses it at the core of &lt;a href=&quot;https://www.mapbox.com/tilemill/&quot;&gt;tilemill&lt;/a&gt;.  In fact, &lt;a href=&quot;https://www.mapbox.com/blog/vector-tile-adoption/&quot;&gt;Esri has adopted&lt;/a&gt; this specification in it’s implementation of vector tiles.  If you look at the vector tile spec defined by mapbox, you will also see various &lt;a href=&quot;https://github.com/mapbox/vector-tile-spec/wiki/Implementations&quot;&gt;applications and tools that implement it&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Within &lt;a href=&quot;https://developers.arcgis.com/javascript/jsapi/&quot;&gt;Esri’s current JavaScript API&lt;/a&gt;, I don’t see support for vector tiles but I’m sure the next release will include it.  OpenLayers  does support &lt;a href=&quot;http://openlayers.org/en/v3.3.0/apidoc/ol.source.TileVector.html&quot;&gt;a TileVector&lt;/a&gt; format, as illustrated in &lt;a href=&quot;http://openlayers.org/en/v3.3.0/examples/tile-vector.html&quot;&gt;this example&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are various tile-generation libraries available for creating vector tiles, and according to Esri’s announcement, a toolbox will be available directly though their Desktop (or Pro) software.  One nice example I found was created by &lt;a href=&quot;https://github.com/glob3mobile&quot;&gt;Glob3 Mobile&lt;/a&gt; called &lt;a href=&quot;https://github.com/glob3mobile/mmt-vector-tiles&quot;&gt;mmt-vector-tiles&lt;/a&gt; which can generate vector tiles directly from a PostGIS database.  They also have a &lt;a href=&quot;http://wb.glob3mobile.com/vl/index_lux.html&quot;&gt;good example&lt;/a&gt; posted of this in action with OpenLayers 3.  If you look at the &lt;a href=&quot;http://igosoftware.dyndns.org:8000/vectorial/lux_buildings_LEVELS_12-18_MERCATOR/GEOJSON/&quot;&gt;file stucture&lt;/a&gt; of their tiles, it looks similar to what is shown in &lt;a href=&quot;http://video.esri.com/watch/4215/smart-mapping-with-vector-map-tiles&quot;&gt;Esri’s presentation&lt;/a&gt; although I’m not certain it adheres to the same specification.&lt;/p&gt;

&lt;p&gt;If you haven’t heard of vector tiles before, I hope this post has been useful!&lt;/p&gt;

</description>
        <pubDate>Sun, 15 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://tannergeo.com/blog/site//vector_tiles/formats/2015/03/15/A-Brief-Look-At-Vector-Tiles.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//vector_tiles/formats/2015/03/15/A-Brief-Look-At-Vector-Tiles.html</guid>
        
        
        <category>vector_tiles</category>
        
        <category>formats</category>
        
      </item>
    
      <item>
        <title>Visualizing February 2015 Weather Data with Turf.js</title>
        <description>&lt;p&gt;February was a particularly cold month across most of the United States.  For us here in Oregon, we’ve been enjoying warm weather… but most further east have had record lows and heavy snow.  I’ve been interested in finding new ways to visualize this data.  I’ll be using &lt;a href=&quot;http://cdo.ncdc.noaa.gov/qclcd/QCLCD?prior=N&quot;&gt;NOAA’s quality controlled logical climatological data&lt;/a&gt; to load weather station data across the US into a Postgres database.  From there I hope to generate TIN’s using &lt;a href=&quot;http://turfjs.org/&quot;&gt;turf.js&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The initial hurdle is getting the stations and readings into our database.  Our file from NOAA has the following pipe delineated headers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WBAN|WMO|CallSign|ClimateDivisionCode|ClimateDivisionStateCode|ClimateDivisionStationCode|Name|State|Location|Latitude|Longitude|GroundHeight|StationHeight|Barometer|TimeZone
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I don’t need all of these, just the WBAN (unique id of station), State, Location, Latitude and Longitude.  Let’s create the initial table:&lt;/p&gt;

&lt;p&gt;Enable postgis on database:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE EXTENSION postgis;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create table for stations:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TABLE weather_stations (
    WBAN integer,
    State char(2),
    Location varchar(150),
    Latitude decimal(9,6) NOT NULL,
    Longitude decimal(9,6) NOT NULL
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I deleted the erroneous columns from the original file for easy import.  From psql command line, I ran the &lt;a href=&quot;http://stackoverflow.com/questions/2987433/how-to-import-csv-file-data-into-a-postgres-table&quot;&gt;following command&lt;/a&gt; to bring the stations into postgres:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;\copy weather_stations FROM &#39;csv location&#39; DELIMITER &#39;|&#39; CSV HEADER;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add an auto incremented primary key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALTER TABLE weather_stations ADD COLUMN id SERIAL PRIMARY KEY;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;http://postgis.net/docs/AddGeometryColumn.html&quot;&gt;Add geometry column&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT AddGeometryColumn (&#39;public&#39;, &#39;weather_stations&#39;, &#39;geom&#39;, 4326, &#39;POINT&#39;, 2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, let’s create actual points from our Latitude and Longitude fields:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;UPDATE weather_stations SET
geom = ST_SetSRID(ST_MakePoint(Longitude, Latitude),4326)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we look at our data in QGIS, we see this now!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/points_qgis.png&quot; alt=&quot;qgis screenshpt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that the stations are in our database, we need to import our weather station readings.  The process will be similar to what we accomplished getting the stations in.  The readings are comma delineated and contain information or daily recordings.  The initial format looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WBAN,YearMonthDay,Tmax,TmaxFlag,Tmin,TminFlag,Tavg,TavgFlag,Depart,DepartFlag,DewPoint,DewPointFlag,WetBulb,WetBulbFlag,Heat,HeatFlag,Cool,CoolFlag,Sunrise,SunriseFlag,Sunset,SunsetFlag,CodeSum,CodeSumFlag,Depth,DepthFlag,Water1,Water1Flag,SnowFall,SnowFallFlag,PrecipTotal,PrecipTotalFlag,StnPressure,StnPressureFlag,SeaLevel,SeaLevelFlag,ResultSpeed,ResultSpeedFlag,ResultDir,ResultDirFlag,AvgSpeed,AvgSpeedFlag,Max5Speed,Max5SpeedFlag,Max5Dir,Max5DirFlag,Max2Speed,Max2SpeedFlag,Max2Dir,Max2DirFlag
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously, we are only interested in the WBAN (station id), YearMonthDay, Tmax, Tmin and Tavg.  These will give us the ability to map low, high and average daily temperatures.  I went ahead and removed the other columns from the initial dataset.  Let’s create the table in postgres.  I used varchar for the temperatures because of some string M values which equal missing… we can filter these out later.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TABLE readings_feb (
    WBAN integer,
    YearMonthDay integer NOT NULL,
    Tmax varchar(5),
    Tmin varchar(5),
    Tavg varchar(5)
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Import csv file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;\copy readings_feb FROM &#39;D:\TannerGeo\WeatherTinTurf\data_raw\QCLCD201502\201502daily.csv&#39; DELIMITER &#39;,&#39; CSV HEADER;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, add a primary key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALTER TABLE readings_feb ADD COLUMN id SERIAL PRIMARY KEY;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To visualize the data in QGIS, I created a special view with just the low temp.  Notice we’ve casted the &lt;a href=&quot;http://dba.stackexchange.com/questions/3429/how-can-i-convert-from-double-precision-to-bigint-with-postgresql&quot;&gt;tmin to int&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW feb_weather AS
SELECT a.id, a.wban, a.yearmonthday, a.tmin::int, b.state, b.location, b.geom
FROM readings_feb a
LEFT JOIN weather_stations b
ON a.wban = b.wban
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By adding a filter for just February 1st in QGIS and symbolizing by temperature, the cold weather trends become apparent.  The mid-west and northeast have bone chilling lows while the west coast and southern US had relatively high temperatures.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/temp_feb01.png&quot; alt=&quot;February Map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To incorporate the data into our mapping application, it’s preferable that we work with a data format suited to our needs.  I’ll be using &lt;a href=&quot;http://geojson.org/&quot;&gt;geojson&lt;/a&gt;, mostly because this format will work best with &lt;a href=&quot;http://turfjs.org/static/docs/global.html#GeoJSON&quot;&gt;turf.js&lt;/a&gt;.  We could &lt;a href=&quot;http://www.postgresonline.com/journal/archives/267-Creating-GeoJSON-Feature-Collections-with-JSON-and-PostGIS-functions.html&quot;&gt;build a query&lt;/a&gt; to generate this straight from postgres, but I’d prefer to just use &lt;a href=&quot;http://www.bostongis.com/pgsql2shp_shp2pgsql_quickguide.bqg&quot;&gt;pgsql2shp&lt;/a&gt; and then &lt;a href=&quot;http://www.gdal.org/ogr2ogr.html&quot;&gt;GDAL’s ogr2ogr&lt;/a&gt; to convert to geojson.&lt;/p&gt;

&lt;p&gt;Create shapefile using pgsql command line tools:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pgsql2shp -f weather_02012015 -h localhost -p 5432 -u username -P password geodata &quot;SELECT * FROM feb_weather WHERE yearmonthday = 20150201&quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Convert to GeoJSON using &lt;a href=&quot;http://www.gisinternals.com/query.html?content=filelist&amp;amp;file=release-1800-x64-gdal-1-11-1-mapserver-6-4-1.zip&quot;&gt;GDAL&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ogr2ogr -f GeoJSON ../geojson/02012015.geojson 02012015.shp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our output file is 287KB could probably be smaller since we don’t necessarily need all of the fields to make the TIN map.&lt;/p&gt;

&lt;p&gt;The only dependencies I know of that we need to create the map are turf.js and leaflet.  I chose to use bower to install these dependencies:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bower install turf
bower install leaflet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For bringing in the geojson, I’m just going to assign the object to a variable and place it in a seperate .js file.   For future reference though, &lt;a href=&quot;https://github.com/calvinmetcalf&quot;&gt;calvinmetcalf&lt;/a&gt; has created a great helper for leaflet called the &lt;a href=&quot;https://github.com/calvinmetcalf/leaflet-ajax&quot;&gt;leaflet-ajax&lt;/a&gt; library for working with geojson files.&lt;/p&gt;

&lt;p&gt;We now have a .js file called 02012015.js with the geojson assigned to the variable feb01.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var feb01 = {
    type&quot;: &quot;FeatureCollection&quot;,
	&quot;crs&quot;: { &quot;type&quot;: &quot;name&quot;, &quot;properties&quot;: { &quot;name&quot;: &quot;urn:ogc:def:crs:OGC:1.3:CRS84&quot; } },
	...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I want to first make sure I can create the TIN’s.  To test, I use turf.js to build and add them to the map with no fill.  By passing in TMIN, turf generates Z values that we can then use to generate our fill colors.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var tin = turf.tin(feb01, &#39;TMIN&#39;);
var geojson = L.geoJson(tin, {
	style : {
		weight : 1
	}
});
geojson.addTo(map);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our result is a visually interesting graphic:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/tin_nofill.png&quot; alt=&quot;tin image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For our fill colors, I’m going to use a library I built called &lt;a href=&quot;https://github.com/tannerjt/classybrew&quot;&gt;classybrew&lt;/a&gt;.  It uses the natural breaks (Jenks) method of statistical classification and applies colors based on Cynthia Brewers color palettes.  I added a new diverging color palette for red-yellow-blue.  This should give us a good thematic perspective of color.&lt;/p&gt;

&lt;p&gt;To stylize the map, we first need to initialize and setup or statistical breaks and color palette based on our minimum temperature values.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// create classification for colors
var brew = new classyBrew();
var values = []; // tmin values
for( var i = 0; i &amp;lt; feb01.features.length; i++) {
	values.push(feb01.features[i].properties[&#39;TMIN&#39;]);
}
brew.setSeries(values);
brew.setNumClasses(6);
brew.classify();
brew.setColorCode(&quot;BuYlRd&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have the classybrew object setup, we can request a color code based on a provided value with &lt;code&gt;getColorInRange(value)&lt;/code&gt;.  We’ll use this function to return a color based on the average of minimum temperature from each station that makes up our triangles.  This can be done during instantiation of our geojson layer.  First, however we need to create our TIN from our points using &lt;code&gt;turf.tin(geojson, z value)&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var tin = turf.tin(feb01, &#39;TMIN&#39;);
var geojson = L.geoJson(tin, {
	style : function (f) {
		return {
			weight : 1,
			fillColor : (function () {
				return brew.getColorInRange((f.properties.a + f.properties.b + f.properties.c) / 3.0, true);
			}()),
			fillOpacity : 0.85
		}
	}
});
geojson.addTo(map);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our result is rather captivating:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/tin_fill.png&quot; alt=&quot;Weather TIN Map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we look at a map from the NOAA National Climatic Data Center, we can see our map aligns with their model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/noaa_temp.png&quot; alt=&quot;NOAA NCDC Map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I think this is all I was looking to do for now.  The output is visually interesting and shows how powerful turf.js is with very little code.  I’d like to take this further and make a more in depth application with the data and turf.js.  Look for more to come in the future, and thanks for following along!&lt;/p&gt;
</description>
        <pubDate>Sun, 01 Mar 2015 00:00:00 -0800</pubDate>
        <link>http://tannergeo.com/blog/site//turfjs/leaflet/postgresql/postgis/noaa/2015/03/01/Visualizing-February-2015-Weather-Date-with-Turf.js.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//turfjs/leaflet/postgresql/postgis/noaa/2015/03/01/Visualizing-February-2015-Weather-Date-with-Turf.js.html</guid>
        
        
        <category>turfjs</category>
        
        <category>leaflet</category>
        
        <category>postgresql</category>
        
        <category>postgis</category>
        
        <category>noaa</category>
        
      </item>
    
  </channel>
</rss>
