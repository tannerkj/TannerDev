<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TannerGeo Blog</title>
    <description>TannerGeo is passionate about computer science, web development and everything geospatial!  We appreciate the helpful tutorials and blog posts others have created.  We would like to give back as well by posting our  experiences and tutorials on new technologies.
</description>
    <link>http://tannergeo.com/blog/site//</link>
    <atom:link href="http://tannergeo.com/blog/site//feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 20 Oct 2017 17:35:59 -0500</pubDate>
    <lastBuildDate>Fri, 20 Oct 2017 17:35:59 -0500</lastBuildDate>
    <generator>Jekyll v3.6.0</generator>
    
      <item>
        <title>ArcGIS API for Python on Mac</title>
        <description>&lt;p&gt;Mac’s currently ship with Python 2.7 as the default install.  The &lt;a href=&quot;https://developers.arcgis.com/python/guide/system-requirements/&quot;&gt;ArcGIS API for Python&lt;/a&gt; requires at least Python 3.5.  However, I don’t want to remove 2.7, but rather have the ability to switch between multiple versions of Python.&lt;/p&gt;

&lt;p&gt;This is the main goal of &lt;a href=&quot;https://github.com/pyenv/pyenv&quot;&gt;pyenv&lt;/a&gt;.  It is much like ruby’s environment manager (&lt;em&gt;rbenv&lt;/em&gt;).  You can change the version of python on a&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;per user basis&lt;/li&gt;
  &lt;li&gt;per project basis&lt;/li&gt;
  &lt;li&gt;with an environment variable&lt;/li&gt;
  &lt;li&gt;globally&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pyenv, however, is not like virtualenv, so we will want to still manage our own virtual environments with &lt;a href=&quot;https://github.com/pyenv/pyenv-virtualenv&quot;&gt;pyenv-virtualenv&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;install&quot;&gt;Install&lt;/h2&gt;

&lt;p&gt;I am using homebrew, a package manager for Mac. (&lt;a href=&quot;https://github.com/pyenv/pyenv#homebrew-on-mac-os-x&quot;&gt;see docs&lt;/a&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew update
$ brew install pyenv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can view a full list of Python versions available to install through pyenv &lt;a href=&quot;https://github.com/pyenv/pyenv/blob/master/COMMANDS.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, to configure pyenv peroperly, add this to your &lt;em&gt;.bash_profile&lt;/em&gt; and restart your terminal.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;if which pyenv &amp;gt; /dev/null; then eval &quot;$(pyenv init -)&quot;; fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s go ahead and install python 3.5.0.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pyenv install 3.5.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we wanted to uninstall, we could do:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pyenv uninstall 3.5.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;set-default-version-of-python&quot;&gt;Set Default Version of Python&lt;/h2&gt;

&lt;h3 id=&quot;golbal&quot;&gt;Golbal&lt;/h3&gt;

&lt;p&gt;Used in all shells&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pyenv global 3.5.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;local&quot;&gt;Local&lt;/h3&gt;

&lt;p&gt;For specific application, writes to file called &lt;em&gt;.python-version&lt;/em&gt;.  This overrides the global version for current directory and all sub-directories.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pyenv local 3.5.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;shell&quot;&gt;Shell&lt;/h3&gt;

&lt;p&gt;Current shell session&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pyenv shell 3.5.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;anaconda&quot;&gt;Anaconda&lt;/h3&gt;

&lt;p&gt;Anaconda is a package manager, environment manager, and Python distribution.&lt;/p&gt;

&lt;p&gt;I’m going to install minconda, which is a slimmed down version of anaconda.  Lets get the latest version.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pyenv install miniconda3-latest
$ pyenv local miniconda3-latest
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;work-within-a-virtual-environment&quot;&gt;Work within a virtual environment&lt;/h3&gt;

&lt;p&gt;To keep things clean and orderly, let’s work within a Python virtural environment by installing &lt;a href=&quot;https://github.com/pyenv/pyenv-virtualenv&quot;&gt;pyenv-virtualenv&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ brew install pyenv-virtualenv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and add the following to our &lt;em&gt;.bash_profile&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ eval &quot;$(pyenv init -)&quot;
$ eval &quot;$(pyenv virtualenv-init -)&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should automatically activate out configured python environment when we navigate to the project directory.  This is not required, but really useful.&lt;/p&gt;

&lt;p&gt;Now, we can create a virtualenv for a specific python version.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pyenv virtualenv miniconda3-latest my-conda-venv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will create a new virtual environment based on the miniconda3-latest Python install in pyenv.&lt;/p&gt;

&lt;h3 id=&quot;install-arcgis-package&quot;&gt;Install &lt;strong&gt;arcgis&lt;/strong&gt; package&lt;/h3&gt;

&lt;p&gt;First, lets activate our newly created virtual environement based on miniconda.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pyenv activate my-conda-venv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s download and install the ArcGIS API in our conda environment.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ conda install -c esri arcgis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can test the installation but trying to import the GIS package.  You can run an interactive shell by typing &lt;strong&gt;python&lt;/strong&gt; into your termnial.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import arcgis.gis from GIS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When we want to deactivate our Python virtual environment, we can simply type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ python deactivate
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;jupyter-notebook&quot;&gt;Jupyter Notebook&lt;/h3&gt;

&lt;p&gt;Jupyter Notebooks are a popular way to visualize and share your code in a readable informative manner.  Let’s also install jupyter with our miniconda environment.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ conda install jupyter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we can run our Jupyter notebook in the browser.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a new file and test your setup with the following example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from arcgis.gis import GIS
gis = GIS()
gis.map()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;If everythign worked properly, you should have now done some really cool things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Used pyenv to manage different installations of Python&lt;/li&gt;
  &lt;li&gt;Managed virtual environments with pyenv-virtualenv&lt;/li&gt;
  &lt;li&gt;Installed miniconda, the lightweight version of anaconda&lt;/li&gt;
  &lt;li&gt;Used the conda package manager to install the Python for ArcGIS library&lt;/li&gt;
  &lt;li&gt;Installed and ran a local Jupyter notebook server&lt;/li&gt;
  &lt;li&gt;Built a small example notebook showcasing the power of Jupyter notebook and ArcGIS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, we can confidently play around with new packages and versions of Python without breaking any of the global settings on your mac.&lt;/p&gt;

&lt;p&gt;Awesome!&lt;/p&gt;
</description>
        <pubDate>Wed, 18 Oct 2017 00:00:00 -0500</pubDate>
        <link>http://tannergeo.com/blog/site//arcgis/python/2017/10/18/ArcGIS-API-for-Python-on-Mac.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//arcgis/python/2017/10/18/ArcGIS-API-for-Python-on-Mac.html</guid>
        
        
        <category>arcgis</category>
        
        <category>python</category>
        
      </item>
    
      <item>
        <title>Use GDAL With Docker</title>
        <description>&lt;p&gt;In this post, we’re going to convert a geojson file to a shapefile using a prebuilt Docker image that already has GDAL installed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.gdal.org/&quot;&gt;GDAL&lt;/a&gt; is the library that most geospatial professionals use for working with geospatial data outside of traditional GUI desktop applications.  One of the most used features (at least for me) is the ability to convert spatial file types with &lt;a href=&quot;http://www.gdal.org/ogr2ogr.html&quot;&gt;ogr2ogr&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, you could install GDAL on your own machine and configure everything to work via command line, but latelty I’ve been seeing the benifits of using isolated environments for specific workflows.&lt;/p&gt;

&lt;p&gt;One way to have a isolated environment for working with GDAL is to use &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; is like having a virtual machine that has all of its resource isolated from its’ surroundings.  However, unlike a virtual machine, Docker containers virtualize the host’s operating system and not the hardware.  A &lt;a href=&quot;https://www.docker.com/what-container&quot;&gt;container image&lt;/a&gt; is a lightweight, stand-alone, executable package of a piece of software that includes everything needed to run it: code, runtime, system tools, system libraries, settings.&lt;/p&gt;

&lt;p&gt;So, why mess with the setup of our own pristine machine when we can tinker and break an ephemeral enviroment without a care in the world?  Docker is like having an unlimited supply of new computers with different operating systems and setups.  You can pull from predefined images, or make your own from scratch that can inherit from existing images.&lt;/p&gt;

&lt;p&gt;Enough with trying to explain Docker.  It’s awesome.  It works.  It will make you a better programmer.  It will save you the headache of breaking your local computer when installing and trying new things.&lt;/p&gt;

&lt;h2 id=&quot;step-1-install-docker&quot;&gt;Step 1: Install Docker&lt;/h2&gt;

&lt;p&gt;Assuming you don’t already have it installed, you can get it work &lt;a href=&quot;https://www.docker.com/docker-mac&quot;&gt;Mac&lt;/a&gt;, or &lt;a href=&quot;https://www.docker.com/docker-windows&quot;&gt;Windows 10&lt;/a&gt; on their official website.  The install should be seamless if you have an updated mac or windows machine.  Older windows machines (&amp;lt;Windows 10) will need to go the &lt;a href=&quot;https://www.docker.com/products/docker-toolbox&quot;&gt;Docker Toolbox&lt;/a&gt; route, which requires more initial setup.&lt;/p&gt;

&lt;h2 id=&quot;step-2--pull-the-gdal-image&quot;&gt;Step 2:  Pull the GDAL Image&lt;/h2&gt;

&lt;p&gt;I’m using this &lt;a href=&quot;https://hub.docker.com/r/geodata/gdal/&quot;&gt;this image&lt;/a&gt; from &lt;a href=&quot;https://github.com/geo-data&quot;&gt;geodata&lt;/a&gt;, which is an Ubuntu (trusty) OS that already has GDAL installed and configured.&lt;/p&gt;

&lt;p&gt;From the command line, type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ docker pull geodata/gdal
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will download the image and allow us to build a container from this image.  Once complete (may take a couple minutes), you can verify it is available by listing your docker images:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ docker image ls
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;step-3--prepare-data-volume&quot;&gt;Step 3:  Prepare Data Volume&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.docker.com/engine/admin/volumes/volumes/&quot;&gt;Volumes&lt;/a&gt; in Docker are a way to share and persist data from your containers.  Containers are inherently ephemeral, and data goes &lt;em&gt;poof&lt;/em&gt; once you remove a container if you are not storing data to a volume.  Volumes also allow you to share data between containers and between your local machine and a running container.&lt;/p&gt;

&lt;p&gt;Create a new directory:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cd ~/documents/
$ mkdir mydata &amp;amp;&amp;amp; cd $_
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Put some geojson into it.  You can copy this single point geojson if you want:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{&quot;type&quot;:&quot;FeatureCollection&quot;,&quot;features&quot;:[{&quot;type&quot;:&quot;Feature&quot;,&quot;properties&quot;:{},&quot;geometry&quot;:{&quot;type&quot;:&quot;Point&quot;,&quot;coordinates&quot;:[-123.19158554077147,44.851122784247245]}}]}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If using the &lt;em&gt;geojson&lt;/em&gt; from above, echo the entire contents into a new file named &lt;strong&gt;point.geojson&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# ~/documents/mydata/
$ echo '{&quot;type&quot;:&quot;FeatureCollection&quot;,&quot;features&quot;:[{&quot;type&quot;:&quot;Feature&quot;,&quot;properties&quot;:{},&quot;geometry&quot;:{&quot;type&quot;:&quot;Point&quot;,&quot;coordinates&quot;:[-123.19158554077147,44.851122784247245]}}]}' &amp;gt; point.geojson
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will need the path to this directory, so lets print the current working directory and copy it to the clipboard:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ pwd
# /Users/tannergeo/Documents/mydata
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;run-the-container-with-gdal-in-interactive-mode&quot;&gt;Run the Container with GDAL in Interactive Mode&lt;/h2&gt;

&lt;p&gt;We are going to do a lot in one command, so let me explain after:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ docker run -it --rm -v /Users/tannergeo/Documents/mydata:/root/mydata/ geodata/gdal /bin/bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is what is going on.  We’re building a container from the image and interacting with it directly.  We’re also mounting our &lt;em&gt;mydata&lt;/em&gt; folder into the container so that the files are shared between them.  For reference:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;-it&lt;/code&gt; - interactive ttyl&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;--rm&lt;/code&gt; - remove the container after we exit it&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;-v&lt;/code&gt; - the volume to mount &lt;em&gt;/from/host/&lt;/em&gt; : &lt;em&gt;/on/container&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;run-ogr2ogr-on-container&quot;&gt;Run ogr2ogr on Container&lt;/h2&gt;

&lt;p&gt;Now that we’re on the Unbuntu machine that has GDAL already setup, we can run our ogr2ogr command.  First, lets verify volume did indeed mount:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# on container
# should looke like
# root@someid93889s: ~/
$ cd /root/
$ ls
mydata
$ cd mydata &amp;amp;&amp;amp; ls
point.geojson
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Awesome, now anything in this folder will be available both by the container and your local machine.  Let’s convert this file to a shiny new shapefile!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#/root/mydata/
$ ogr2ogr -f &quot;ESRI Shapefile&quot; point_shp.shp &quot;point.geojson&quot;
$ ls
point.geojson point_shp.dbf point_shp.prj point_shp.prj point_shp.shx
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;exit-container-and-see-shapefile-on-local-machine&quot;&gt;Exit Container and See Shapefile on Local Machine&lt;/h2&gt;

&lt;p&gt;Let’s now exit the contianer and see how the output was shared on our local machine.  Since we flagged &lt;code&gt;--rm&lt;/code&gt;, the container will be removed, but the data will persist.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;type &lt;strong&gt;exit&lt;/strong&gt; to go back to your terminal&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#~/documents/mydata/
$ root@someid93889s:exit
$ ls
point.geojson point_shp.dbf point_shp.prj point_shp.prj point_shp.shx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Without messing with our own machine, we were able to convert a geospatial geojson file to a shapefile on an already configured Ubuntu machine with GDAL.  These isolated environments are really a great utility, especially if you’re prone to breaking your computer when messing with your own configurations (like I do often).  I might never install new libraries locally again!&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; has a variety of different uses, not just running commands in an isolated environment.  It can be used to replicate development environments across an entire team.  Docker also helps to automatically scale apps in cloud environments.&lt;/p&gt;

&lt;p&gt;There are many other great tutorials on Docker if you want to continue learning, since this was a short and sweet tutorial lacking many of the great features that Docker has to offer.  Here are a few:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.docker.com/get-started/&quot;&gt;Dockers Official Getting Started Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/docker/labs&quot;&gt;Docker Lab Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 05 Oct 2017 00:00:00 -0500</pubDate>
        <link>http://tannergeo.com/blog/site//docker/gdal/2017/10/05/Use-GDAL-With-Docker.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//docker/gdal/2017/10/05/Use-GDAL-With-Docker.html</guid>
        
        
        <category>docker</category>
        
        <category>gdal</category>
        
      </item>
    
      <item>
        <title>Adding ArcGIS Sign In Using Omniauth and Rails</title>
        <description>&lt;p&gt;We’re all familiar with being able to sign into other applications with our Facebook
or Google account, but what about an ArcGIS Online account? This is possible because
ArcGIS Online uses OAuth 2 as it’s authentication framework.  There is limited information on
how this process works, so this post is meant to shed some light into the process.&lt;/p&gt;

&lt;p&gt;For this tutorial, we will create a dummy application that will allow users to sign in using their existing
ArcGIS Online credentials.  Once signed in, the user will be able to post a message. Pretty simple!&lt;/p&gt;

&lt;p&gt;The process assumes you have a working knowledge of Ruby on Rails and a basic understanding of OAuth and authentication
workflows in Rails.&lt;/p&gt;

&lt;h2 id=&quot;step-1-application-setup&quot;&gt;Step 1. Application Setup&lt;/h2&gt;

&lt;p&gt;We must first create our rails application using the command line rails generator:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rails new MySecretApp
cd MySecretApp
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;step-2-generate-models&quot;&gt;Step 2. Generate Models&lt;/h2&gt;

&lt;p&gt;Our basic application only needs two data models; &lt;code&gt;User&lt;/code&gt; and  &lt;code&gt;Message&lt;/code&gt;.  Let’s
create the &lt;code&gt;message&lt;/code&gt; model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rails generate scaffold message body:text
rils db:migrate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We used &lt;code&gt;generate scaffold&lt;/code&gt; because we also want the controller and corresponding views
to be created automatically.&lt;/p&gt;

&lt;p&gt;Our message currently only has one field (body) of type text.  This will be the body of the
message that an authenticated user will will write.&lt;/p&gt;

&lt;p&gt;Next, we need to create our user model.  To do that, we will use a gem called Devise, which will
also handle our authentication workflows.  Place the following gem in your Gemfile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gem 'devise', '~&amp;gt; 4.2'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and run &lt;code&gt;bundle install&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Run the Devise generator:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rails genererate devise:install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We need to setup the default URL for our Devise mailer in our development environment:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# /config/environments/development.rb
config.action_mailer.default_url_options = { host: 'localhost', port: 3000 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, we need to generate our &lt;code&gt;user&lt;/code&gt; model using a Devise generator.  This will create
all of the necessary fields for handling authentication.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rails generate devise user
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To persist our changes to the database, run the database migration&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rails db:migrate
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;step-3-configure-associations&quot;&gt;Step 3. Configure Associations&lt;/h2&gt;

&lt;p&gt;For active record to know about the association between our user and their corresponding messages,
we need to add the association to our models:&lt;/p&gt;

&lt;p&gt;A message belongs_to a user:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# /app/models/message.rb
class Message &amp;lt; ApplicationRecord
  belongs_to :user
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A user has many message.  If a user is deleted, all dependent messages are also removed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# app/models/user.rb
class User &amp;lt; ApplicationRecord
  has_many :messages, dependent: :destroy
  ...
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To associate a message with a particular user, we need to add the user as a foreign key
to the message model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rails generate migration add_user_to_message user:references
rails db:migrate
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;step-4-wiring-everything-together&quot;&gt;Step 4. Wiring Everything Together&lt;/h2&gt;

&lt;p&gt;So far, we’ve added our message scaffold, created an authenticated user model, and hooked up the
associations for our active directory to work as intended.  This was all done with the help
of Rails generators, and the Devise authentication gem.  Now, we need to tie all these parts
together for a working application.&lt;/p&gt;

&lt;p&gt;When a user visits the site, the home page should show a list of posted messages, which
requires us to set the root route:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# config/routes.rb
...
root 'messages#index'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we need to modify our app to accomodate the security measures created by adding our Devise user
model.  First, let’s restrict the ability to create, update, or delete messages unless a user is
signed in:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# /app/controllers/messages_controller.rb
before_action :authenticate_user!, except: [:index, :show]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This limits any unauthenticated user to only access the index and show actions of the messages
controller.  Next, we need to modify some of the actions to associate the messages with the current
user.  Modify the actions in the messages controller:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# /app/controllers/messages_controller.rb
def new
  @message = current_user.messages.build
end
def create
  @message = current_user.messages.build(message_params)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, we need to add some navigation links to our main application template for signing in and
signing out &lt;code&gt;app/layouts/application.html.erb&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;body&amp;gt;
  &amp;lt;% if user_signed_in? %&amp;gt;
    &amp;lt;%= link_to &quot;Sign Out&quot;, destroy_user_session_path, method: :delete %&amp;gt;
  &amp;lt;% else %&amp;gt;
    &amp;lt;%= link_to &quot;Sign In&quot;, new_user_session_path %&amp;gt;
    &amp;lt;%= link_to &quot;Sign Up&quot;, new_user_registration_path %&amp;gt;
  &amp;lt;% end %&amp;gt;
  &amp;lt;%= yield %&amp;gt;
&amp;lt;/body&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have a working application that has built in authentication.  Give it a try by running
&lt;code&gt;rails s&lt;/code&gt; and testing your app.&lt;/p&gt;

&lt;h2 id=&quot;step-5-sign-in-with-arcgis&quot;&gt;Step 5. Sign in With ArcGIS&lt;/h2&gt;

&lt;p&gt;When we’re signing in now, were doing so by creating a new account.  What we want to do, is avoid this by
allowing users to sign in with their existing ArcGIS Online accounts.  Devise supports this by adding
OmniAuth.  The first step is to add the ArcGIS OmniAuth and support for OAuth2 gem to our Gemfile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gem 'omniauth-oauth2', '~&amp;gt; 1.3.1'
gem 'omniauth-arcgis', '~&amp;gt; 0.1.1'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install gems:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bundle install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We need to update the user model to accomodate the ArcGIS Provider and user id:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rails generate migration add_omniauth_to_users provider:string uid:string
rails db:migrate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to enable OAuth with ArcGIS Online, you need to register the application
on &lt;a href=&quot;http://developers.arcgis.com&quot;&gt;http://developers.arcgis.com&lt;/a&gt;.  After you have registered
the application, add your development URL (http://localhost:3000) to the Redirect URI form
under the authentication tab.  Notice the Client and Client Secret listed on the page?  We will
use these to setup our OmniAuth.&lt;/p&gt;

&lt;p&gt;While you could hard code the Cleint ID an Secret into your initializer, I like to use
&lt;a href=&quot;https://github.com/laserlemon/figaro&quot;&gt;Figaro&lt;/a&gt; to manage the setting of my environment variables.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gem 'figaro', '~&amp;gt; 1.1', '&amp;gt;= 1.1.1'
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;bundle install
bundle exec figaro install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This adds a new file &lt;code&gt;config/application.yml&lt;/code&gt; and adds it to our &lt;code&gt;.gitignore&lt;/code&gt; file.  Add your Client ID
and Secret here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;arcgis_client_id: 'xxxxx'
arcgis_client_secret: 'xxxxx'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will allow us to access ENV[‘arcgis_client_id’] from anywhere in our application.&lt;/p&gt;

&lt;p&gt;Next, declare the ArcGIS provider in &lt;code&gt;config/initializers/devise.rb&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config.omniauth :arcgis, ENV['arcgis_client_id'], ENV['arcgis_client_secret']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make our user model omniauthable:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# /app/models/user.rb
devise :omniauthable, :omniauth_providers =&amp;gt; [:arcgis]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Setup our omniauth callback URL in our routes &lt;code&gt;config/routes.rb&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;devise_for :users, :controllers =&amp;gt; { :omniauth_callbacks =&amp;gt; &quot;omniauth_callbacks&quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add the file for our OmniAUth callbacks that inherits from Devise &lt;code&gt;app/controllers/omniauth_callbacks_controller&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class OmniauthCallbacksController &amp;lt; Devise::OmniauthCallbacksController
  def arcgis
    @user = User.from_omniauth(request.env[&quot;omniauth.auth&quot;])

    # sign_in_and_redirect @user
    if @user.persisted?
      sign_in_and_redirect @user, :event =&amp;gt; :authentication
      set_flash_message(:notice, :success, :kind =&amp;gt; &quot;arcgis&quot;) if is_navigational_format?
    else
      session[&quot;devise.arcgis_data&quot;] = request.env[&quot;omniauth.auth&quot;]
      redirect_to new_user_registration_url
    end
  end

  def failure
    redirect_to root_path
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, we need to add the above &lt;code&gt;from_omniauth&lt;/code&gt; method to our user model&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def self.from_omniauth(auth)
  where(provider: auth.provider, uid: auth.uid).first_or_create do |user|
    user.email = auth.info.email
    user.password = Devise.friendly_token[0,20]
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that’s it! This is a very simple applcation, but it shows how we can use the OAuth2 capabilities of ArcGIS Online to sign into our app.&lt;/p&gt;
</description>
        <pubDate>Thu, 05 Jan 2017 00:00:00 -0600</pubDate>
        <link>http://tannergeo.com/blog/site//rails/omniauth/arcgis/2017/01/05/Adding-ArcGIS-Sign-In-Using-Omniauth-and-Rails.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//rails/omniauth/arcgis/2017/01/05/Adding-ArcGIS-Sign-In-Using-Omniauth-and-Rails.html</guid>
        
        
        <category>rails</category>
        
        <category>omniauth</category>
        
        <category>arcgis</category>
        
      </item>
    
      <item>
        <title>A Quick Look At Aurelia</title>
        <description>&lt;p&gt;I have been a big proponent of AngularJS.  My biggest compaint with any framework however, is the learning curve compared to just jumping into coding with no new &lt;em&gt;stuff&lt;/em&gt; to learn.  I spent years working with Angular, but I am always suprised at how quickly I forget the ins and outs of new language or framework.  Now that I’m preparing to work on a new project, I tried to go back to Angular docs to freshen up.  AngularJS 2.0 is out, which means more things to learn and test out.  Learning is never a bad thing, but re-learning can get tedious and sometimes counterproductive.  I know JavaScript like the back of my hand, shouldn’t a framework feel the same?&lt;/p&gt;

&lt;p&gt;At last, a quick browse around the web led me to discover &lt;a href=&quot;http://aurelia.io/docs.html#/aurelia/framework/1.0.0-beta.1.0.8/doc/article/what-is-aurelia&quot;&gt;Aurelia&lt;/a&gt;.  It’s new (&lt;a href=&quot;http://www.isaacchansky.me/days-since-last-new-js-framework/&quot;&gt;although this is relative&lt;/a&gt;), but already it is up to a production ready Beta 1 release.  It is built using modern ECMAScript 2016 and follows similar concepts we use in all modern MV* frameworks.  Data binding, dependency injection, templating, routing, etc.&lt;/p&gt;

&lt;p&gt;The best thing I noticed so far is that the framework is very discreet.  There aren’t many ‘frameworky’ things to learn and lets you leverage the power of a framework without needing to be intimate with the framework.  If you’ve spent hours re-learning backbone, AngularJS, React, etc… you know what I mean.&lt;/p&gt;

&lt;p&gt;I feel like the Aurelia framework should follow Douglas Crockfords convention of: &lt;strong&gt;AngularJS 2.0 - The Good Parts&lt;/strong&gt; by &lt;strong&gt;Rob Eisenberg&lt;/strong&gt; (the creator of Aurelia).  In fact, it turns out &lt;a href=&quot;https://github.com/EisenbergEffect&quot;&gt;Rob Eisenberg&lt;/a&gt; is actually the driving input behind Angular 2.0 and now known as &lt;a href=&quot;http://eisenbergeffect.bluespire.com/leaving-angular/&quot;&gt;the guy who left Angular&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://aurelia.io/docs.html#/aurelia/framework/1.0.0-beta.1.0.8/doc/article/what-is-aurelia&quot;&gt;Getting Started Guide&lt;/a&gt; on Aurelia provides a straight forward approach to getting introduced to the framework.  Different paths are avaialbe depending on what type of background you are coming from. You can follow along with their &lt;a href=&quot;http://aurelia.io/docs.html#/aurelia/framework/1.0.0-beta.1.0.8/doc/article/getting-started&quot;&gt;getting started guide&lt;/a&gt; on your own using either the ES 2016 or Typescript kit.&lt;/p&gt;

&lt;p&gt;I’m using a &lt;a href=&quot;https://www.npmjs.com/package/http-server&quot;&gt;simple web server&lt;/a&gt; to serve out the web files, since Aurelia is simply client side libraries.  This is also referenced in the guide:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install http-server -g
http-server -o
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The -o flag will open your app in a browser.  You should see a comforting ‘Welcome to Aurelia’ string in your browser.&lt;/p&gt;

&lt;p&gt;I also suggest watching &lt;a href=&quot;https://vimeo.com/131641012&quot;&gt;Rob’s video introducing Aurelia&lt;/a&gt;.  The video intoduces the framework in a very straightforward, digestable manner.  I think this was the point, as is the point of the framework itself.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Final Thoughts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I’ve spent the better part the last day building some sample apps with Aurelia… and I have to say I’m impressed.  This should be my primary framework for the coming year.&lt;/p&gt;
</description>
        <pubDate>Sat, 23 Jan 2016 00:00:00 -0600</pubDate>
        <link>http://tannergeo.com/blog/site//aurelia/javascript/2016/01/23/A-Quick-Look-At-Aurelia.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//aurelia/javascript/2016/01/23/A-Quick-Look-At-Aurelia.html</guid>
        
        
        <category>aurelia</category>
        
        <category>javascript</category>
        
      </item>
    
      <item>
        <title>Spatial Version Control with GeoGig</title>
        <description>&lt;p&gt;There are a lot of concepts that are core to the programming and development communities that have yet to manifest themselves (to the masses) in the GIS world.  One of them is version control.&lt;/p&gt;

&lt;h4 id=&quot;basics&quot;&gt;Basics&lt;/h4&gt;

&lt;p&gt;If you have ever used git or svn, you know of it’s usefulness in maintaining a movable history of the ‘state’ of your code or other files.  If you have never heard of version control, it is simply a system that keeps track of changes and allows you to ‘roll-back’ if needed.  You can visualize these changes (additions and subtractions) and provide readable comments with every &lt;em&gt;commit&lt;/em&gt;.  It also provides methods to resolve &lt;em&gt;conflicts&lt;/em&gt; if multiple changes contradict each other.&lt;/p&gt;

&lt;p&gt;I one had a colleague who explained version control very elegantly.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;‘copy and paste is the one true evil.’ - colleague/friend&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To be honest, I use this all the time when I try to explain the usefulness of version control.  When you copy and paste, you now have two completely separate datasets that know nothing about each other.  When one is updated, the other falls behind and trying to keep them smartly in sync can become futile.  With version control, we can checkout a copy, make changes, and commit them back to the remote repository.  If updates need to be rolled back, we can easily revert them.&lt;/p&gt;

&lt;h4 id=&quot;why-gis-needs-it&quot;&gt;Why GIS Needs It&lt;/h4&gt;
&lt;p&gt;Technically, Esri has been using versioning in enterprise geodatabase solutions for some time now.  It allows concurrent editing of a geodatabase, conflict management, and the ability to revert to a previous state. However, it requires complex setup and the need to be in the Esri environment.&lt;/p&gt;

&lt;p&gt;I’ll be the first to admit my working project folders usually have about 10+ versions of the same dataset.  GIS analysts are always worried about needing to revert their edits or make a backup in the event of bad edits.  When I worked for the City of Charleston, most of my work was with the planning department.  My ‘final changes’ were never actually final and I would gulp when I heard the phrase “let’s go back to the version we used for the last city council meeting…”.  If your like me, you have a folder that looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bike_route_01152014.shp
bike_route_01202014.shp
bike_route_proposal1.shp
bike_route_final.shp
bike_route_final2.shp
bike_route_final_for_real.shp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a one simple example of why we need version control… badly.&lt;/p&gt;

&lt;p&gt;Another reason is making sure your team is always working with the latest data.  Even if everybody is working on a local clone of a spatial dataset, they could run a quick &lt;em&gt;pull&lt;/em&gt; command at anytime and retrieve the most up to date data.&lt;/p&gt;

&lt;p&gt;####Why Not Just Use Git?
Current version control systems like git work great for text.  The flawlessly track what text was added or removed, and this provides the information we need to understand changes.  Spatial data is binary, and although current systems will track the changes, it doesn’t provide us with much insight into what changed.  If we wanted to understand what changes were made in our latest commit, we would only see that a bunch of 1’s and 0’s changed… not much help.  What we need to see is how many features were added, modified, or removed and on which dataset.&lt;/p&gt;

&lt;p&gt;####Geogig Background
&lt;a href=&quot;http://geogig.org/&quot;&gt;GeoGig&lt;/a&gt; (previouly GeoGit) is a distributed version control system (DVCS) for geospatial data.  The ‘distributed’ term is important and follows closer to the architecture of Git than SVN (google git vs svn for more info).  The software is open source, and currently maintained by the &lt;a href=&quot;https://www.locationtech.org/projects/technology.geogig&quot;&gt;Eclipse Foundation&lt;/a&gt; and formerly maintained by BoundlessGeo.  A disclaimer on the GeoGig site states that is in ‘development’ and considered unstable.  In my short experience with the software, it worked great, but I would not yet use this in a full production environment.&lt;/p&gt;

&lt;p&gt;I would love to see &lt;a href=&quot;http://geogig.org/&quot;&gt;GeoGig&lt;/a&gt; developed to be stable enough to use as a reliable tool for spatial data version control.&lt;/p&gt;

&lt;p&gt;####GeoGig Sample Workflow
I would recommend following the &lt;a href=&quot;http://geogig.org/#install&quot;&gt;Getting Started&lt;/a&gt; guide for downloading and getting up and running with GeoGig.  The following workflow will demonstrate the following scenario:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You are working in a team environment editing bike routes.  The shapefile needs to be accessible on your local computer for when you are editing in the field and taking work home.  You also need to push your changes to a shared network drive and pull down changes that other editors are doing.  I will assume you are working in a windows environment.&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;On your (S:) shared drive, initialize a GeoGig repository.  This is where your master repo will live, and will act like a library for users to check out and back in their edits.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; S:master_routes&amp;gt; geogig init
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This will create a .geogig directory that will track all of your changes, among other things.  You will need to import your bike_routes shapefile which can be anywhere.  For this example I will assume it’s in the same folder as your newly created GeoGig repository.  Think of this step as putting it into a warehouse.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; S:master_routes&amp;gt; geogig shp import ./bike_routes.shp 
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now that it is in the warehouse, you need add it to a &lt;em&gt;loading dock&lt;/em&gt; - like location before committing it entirely.  Only changes will be added, and in our initial case will be all of our existing features in bike_routes.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; S:master_routes&amp;gt; geogig add 
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is the equivalent of ‘add all changes’ to our loading dock to be shipped from the warehouse.  Next we will commit our added changes, which is like putting it on the truck.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; S:master_routes&amp;gt; geogig commit -m &quot;Add initial commit of bike routes&quot;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The -m trigger allows us to add a message with our commit.  This is helpful to make each commit human readable and easier to use.  Now we want all of our users/editors of this repository to pull down a working repo to their local machines.  For this, our (S:) shared drive will act as a remote repository.  From our (C:) local drive, we can add our connection:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; C:local_bike_routes&amp;gt; geogig clone S:/bike_routes/
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The clone command clones our master repository and automatically creates a remote tracking branch.  In essence, it is a clone that knows how to access it’s master.  It can pull down any changes done to, or push changes up to the master.  Our master remote branch is automatically named &lt;em&gt;origin&lt;/em&gt;.  If you look in your directory you might be confused.  There bike_routes shapefile is nowhere to be found, only a .geogig directory?!?! With GeoGig, everything is stored in the database within the .geogig repository.  If you want to export the shapefile to work on, you will have to do this implicitly:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; C:local_bike_routes&amp;gt; geogig shp export HEAD:bike_routes ./bike_routes.shp
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This exports out a shapefile for you to work on.  You can also export the data out in various different formats (geojson, postgres, spatialite, etc), which can be a great tool in itself.  Now that you have a local shapefile to edit, you can make the changes you need in whatever software you are using (qgis, arcgis, etc).  Once you are ready to push the changes up to the master repository, you will have to use a couple command line steps.  You will need to: import the shapefile into your local repo, stage your edits (loading dock), and commit them (put them on the truck).  Once everything is committed in your local geogig repository, you will push the changes to your remote (origin) branch.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; C:local_bike_routes&amp;gt; geogig shp import ./bike_routes.shp -d bike_routes
 C:local_bike_routes&amp;gt; geogig add
 C:local_bike_routes&amp;gt; geogig commit -m &quot;Add new rutledge ave bike route&quot;
 C:local_bike_routes&amp;gt; geogig push origin master
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We have now updated not only our own bike_routes feature, but also the feature on our S: drive.  All our other editors working on their own local repositories can pull in these changes, and view your comments in the log.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; C:local_bike_routes&amp;gt; geogig pull origin master
 C:local_bike_routes&amp;gt; geogig log
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;####Discussion&lt;/p&gt;

&lt;p&gt;I tried this workflow on a dataset that I need to continually update and share (even through I warned you not to use it in production).  It worked without error and organized my process ten-fold.  Below are things I liked, and some I didn’t.&lt;/p&gt;

&lt;p&gt;Positive:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I liked that it inherits from git.  Most of the commands are the same, although the workflow is slightly different.  This makes the learning curve very minimal for those already familiar with git.&lt;/li&gt;
  &lt;li&gt;The distributed architecture works well for geospatial data&lt;/li&gt;
  &lt;li&gt;Being able to export to different formats is great.&lt;/li&gt;
  &lt;li&gt;The documentation is clear and concise.&lt;/li&gt;
  &lt;li&gt;Fast.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Negative:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No support for file geodatabase.  This means you will have to export to a shapefile to use this utility.&lt;/li&gt;
  &lt;li&gt;I don’t like that I have to import my data before I add and commit it.  Importing and then adding seems redundant and complicates the process.  This part is different from git.&lt;/li&gt;
  &lt;li&gt;This may be lazy, but typing git is easier than typing geogig.  I typed it a lot in my workflow and it would be easier if it defaulted to gig or something simple.  Again, I could just be lazy and this could probably be changed easily.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It would be also nice to see a GitHub-like solution for publishing and collaborating on spatial data.  Overall I liked it and will be using this a lot more in my current work.  I encourage both the use and further development of GeoGig to create a stable solution for geospatial data version control.&lt;/p&gt;

&lt;p&gt;####Extras&lt;/p&gt;

&lt;p&gt;GeoGig has many extras that I didn’t cover in this article that I think others should check out.  Here are a few:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://geogig.org/docs/interaction/osm.html&quot;&gt;Importing OSM Data into GeoGig&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://geogig.org/docs/interaction/console.html&quot;&gt;The GeoGig Console&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://geogig.org/docs/interaction/geoserver_ui.html&quot;&gt;Using GeoGig with GeoServer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://geogig.org/docs/interaction/web-api.html&quot;&gt;Build in Web Server and API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I especially liked the GeoGig console.  This allowed me to more easily write a batch script that exported the latest shapefiles our of my master repository and output a log of all recent commits.&lt;/p&gt;

&lt;h1&gt;#&lt;/h1&gt;

&lt;p&gt;Other Links:
&lt;a href=&quot;http://www.spatiallyadjusted.com/gis-version-control/&quot;&gt;James Fee Article on GIS Version Control&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 04 Sep 2015 00:00:00 -0500</pubDate>
        <link>http://tannergeo.com/blog/site//geogig/git/versioning/gis/data/2015/09/04/Spatial-Version-Control-with-GeoGig.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//geogig/git/versioning/gis/data/2015/09/04/Spatial-Version-Control-with-GeoGig.html</guid>
        
        
        <category>geogig</category>
        
        <category>git</category>
        
        <category>versioning</category>
        
        <category>gis</category>
        
        <category>data</category>
        
      </item>
    
      <item>
        <title>National Parks Map and Image Gallery</title>
        <description>&lt;p&gt;This application and its accompanying tutorial are our tribute to National Park Week (Apr 18-26).  A working knowledge of &lt;a href=&quot;http://www.qgis.org/en/site/&quot;&gt;QGIS&lt;/a&gt;, &lt;a href=&quot;http://leafletjs.com/&quot;&gt;Leaflet&lt;/a&gt;, &lt;a href=&quot;http://turfjs.org/&quot;&gt;Turf.js&lt;/a&gt;, &lt;a href=&quot;https://jquery.com/&quot;&gt;jQuery&lt;/a&gt;, &lt;a href=&quot;http://getbootstrap.com/&quot;&gt;Bootstrap&lt;/a&gt;, &lt;a href=&quot;http://lokeshdhakar.com/projects/lightbox2/&quot;&gt;Lightbox&lt;/a&gt;, and &lt;a href=&quot;https://www.flickr.com/services/api/&quot;&gt;Flickr API&lt;/a&gt; are required to build out this application using this tutorial.&lt;/p&gt;

&lt;h6 id=&quot;data-acquisition-and-conversiongetting-required-modules&quot;&gt;Data Acquisition and Conversion/Getting Required Modules&lt;/h6&gt;

&lt;p&gt;To get started, the first step is to download the &lt;a href=&quot;https://catalog.data.gov/dataset/national-park-boundariesf0a4c&quot;&gt;national parks boundaries dataset&lt;/a&gt; and save to your working directory. Load the shapefile into QGIS to view and, in this case, simplify geometries due to the file size.  To accomplish that in QGIS, select Vector -&amp;gt; Geometry Tools -&amp;gt; Simplify Geometries.&lt;/p&gt;

&lt;p&gt;Next, using command line and in our working directory, convert the shapefile to GeoJSON using &lt;a href=&quot;http://www.gdal.org/&quot;&gt;GDAL&lt;/a&gt;.  In this case we entered:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ogr2ogr -f GeoJSON &amp;lt;output geojson&amp;gt; &amp;lt;input shapefile&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create the application, we will define our dependencies in our &lt;em&gt;package.json&lt;/em&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&quot;dependencies&quot; : {
   	&quot;turf&quot; : &quot;*&quot;,
   	&quot;leaflet&quot; : &quot;*&quot;,
   	&quot;bootstrap&quot; : &quot;*&quot;,
   	&quot;jquery&quot; : &quot;*&quot;
  	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, using command line, use npm to add the appropriate packages locally.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will see your packages in a new directory called &lt;em&gt;node_modules&lt;/em&gt; inside your project folder.&lt;/p&gt;

&lt;p&gt;In your &lt;em&gt;.gitignore&lt;/em&gt; file, add node_modules, and they will be left out when pushing to your &lt;a href=&quot;https://github.com/&quot;&gt;GitHub&lt;/a&gt; repo.&lt;/p&gt;

&lt;h6 id=&quot;building-the-application&quot;&gt;Building the Application&lt;/h6&gt;

&lt;p&gt;Below is an outline of the steps taken to build out this application, along with some code and image samples.  Dig further into the source code for more information.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;add a Leaflet basemap&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  var map,           // main appliation map
      basemap;       // leaflet basemap layer
		
  var app_config = {
  basemap : &quot;http://{s}.tiles.wmflabs.org/bw-mapnik/{z}/{x}/{y}.png&quot;,
  basemapAttribution : &quot;&amp;amp;copy; &amp;lt;a href=\&quot;http://www.openstreetmap.org/copyright\&quot;&amp;gt;OpenStreetMap&amp;lt;/a&amp;gt;&quot;
  };
	
  // create map and add tiled basemap
  map = L.map('map').setView([40.12, -98.57], 4);
  basemap = L.tileLayer(app_config.basemap, {
  attribution: app_config.basemapAttribution
  });
  basemap.addTo(map);
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;filter features to exclude geometries that aren’t parks (rivers, monuments, etc.)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;setup GeoJSON layer style&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  var np_boundaries, // raw geojson
  	np_geo;        // leaflet geojson layer
	
  // filter geojson to only parks
  np_boundaries.features = $.map(np_boundaries.features, function (val, i){
      if(val.properties['UNIT_TYPE'] == 'National Park') {
          return val;
      }
  });
	
  function style(feature) {
      return {
          weight : 1,
          color : 'green',
          fillColor : 'green'
      }
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;setup Flickr configuration&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  // Flickr Configuration
  var flickr_config = {
  maxNumImages : 4, // max number of images to request for each park
  url : &quot;https://api.flickr.com/services/rest&quot;,
  key : &amp;lt;your flickr-provided key&amp;gt;
  };
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;add GeoJSON to map along with initial Flickr images&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  /* GEOJSON LAYER */
  np_geo = L.geoJson(np_boundaries, {
      style : style,
      onEachFeature : function (f, l) {
          var pn =  f.properties['UNIT_NAME'] + &quot; National Park&quot;;
          l.bindPopup(
              &quot;&amp;lt;h3&amp;gt;&quot; + pn + &quot;&amp;lt;/h3&amp;gt;&quot; +
              &quot;&amp;lt;a href='http://en.wikipedia.org/wiki/&quot; + pn.split(&quot; &quot;).join(&quot;_&quot;) + 
      &quot;' target='_blank'&amp;gt;Learn more about this National Park&amp;lt;/a&amp;gt;&quot;, {
              autoPan : false
          });

      }
  });
  np_geo.addTo(map);
  getImages(); // make inital call once
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/natl-park-gallery-1.png&quot; alt=&quot;National Park Gallery Image 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;add events for extent change&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  // setup map events 
  map.on('moveend', function (e) {
      getImages();
  });
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;acquire bounding boxes and park name tags in preparation for Flickr API calls (uses Turf.js)&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  var queue = []; // placeholder if need for queue and digest management
  var cached = {}; // cache park images to limit api calls
  var current_gallery = {}; // images and details in current extent
  var current_req_length = 0;
  var queue_length = 0;
	
  function callFlickr(f, parkName) {
      var bbox = turf.extent(f.geometry).join(&quot;,&quot;);
      var tags = parkName.split(&quot; &quot;);
      tags.push(&quot;park&quot;);
      tags.push(&quot;national&quot;);
      tags = tags.join(&quot;,&quot;);
      $.ajax({
          url : flickr_config.url,
          dataType : 'json',
          data : {
              api_key : flickr_config.key,
              tags : tags,
              bbox : bbox,
              format : 'json',
              method : 'flickr.photos.search',
              per_page : flickr_config.maxNumImages,
              nojsoncallback : 1
          }
      }).done(function (resp) {
          queue_length += 1;
          if(resp.stat !== &quot;ok&quot;) {
              if(queue_length == current_req_length) {
                  buildHtml(parkName)
              }
              return;
          }
          var photos = [];
          $.each(resp.photos.photo, function (i, v) {
              photos.push(&quot;https://farm&quot; + v.farm + &quot;.staticflickr.com/&quot; + v.server + &quot;/&quot; + v.id + &quot;_&quot; + v.secret + &quot;_n.jpg&quot;);
          });
          // add to cache
          cached[parkName] = photos;
          current_gallery[parkName] = photos;
          if(queue_length == current_req_length) {
              buildHtml(parkName);
          }
      });
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;get current extent and clear current gallery&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;calculate the # of matches for image queue and make Flickr API calls&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  function getImages() {
      // get current extent
      var mapBounds = map.getBounds();
      var centroid;
      // clear current gallery
      current_gallery = {};
      current_req_length = 0;
      queue_length = 0;
      // calculate number of matches for queue
      $.each(np_geo.getLayers(), function (i, l) {
          // get centroid of layer
          centroid = turf.centroid(l.feature);
          var latlng = L.latLng([centroid.geometry.coordinates[1], centroid.geometry.coordinates[0]]);
          if(mapBounds.contains(latlng)) {
              current_req_length += 1;
          }
      });
      // Make flickr calls
      $.each(np_geo.getLayers(), function (i, l) {
          // get centroid of layer
          centroid = turf.centroid(l.feature);
          var latlng = L.latLng([centroid.geometry.coordinates[1], centroid.geometry.coordinates[0]]);
          if(mapBounds.contains(latlng)) {
              if(l.feature.properties['UNIT_NAME'] in cached) {
                  // push existing parks with images
                  current_gallery[l.feature.properties['UNIT_NAME']] = cached[l.feature.properties['UNIT_NAME']];
                  queue_length += 1;
                  if(queue_length == current_req_length) {
                      buildHtml(l.feature.properties['UNIT_NAME']);
                  }
              } else {
                  callFlickr(l.feature, l.feature.properties['UNIT_NAME']);
              }
          }
      });
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;add events for mouseover, mouseout, and click (uses Lightbox)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;build html for image carousel&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  function highlightStyle() {
      return {
          weight : 4,
          color : 'red',
          fillColor : 'red'
      }
  }
	
  function highlightPark(park) {
      var park = park;
      $.each(np_geo.getLayers(), function (idx, layer) {
          if (layer.feature.properties['UNIT_NAME'] == park) {
              layer.setStyle(highlightStyle());
              layer.openPopup();
          } else {
              layer.setStyle(style());
          }
      })
  }

  // build html for image carousel
  function buildHtml() {
      var gallery_items;
      var gallery_container = $(&quot;&amp;lt;div /&amp;gt;&quot;, {
          id : 'owl-slider',
          class : 'own-carousel'
      });
      var list = [];
      $.each(current_gallery, function (park, photos) {
          list.push(&quot;&amp;lt;a class='park-item' href='' target='_blank'&amp;gt;&quot; + park + &quot;&amp;lt;/a&amp;gt;&quot;);
          $.each(photos, function (idx, photo) {
              var div = $(&quot;&amp;lt;div /&amp;gt;&quot;, {
                  class : 'thumbnail'
              });

              div.on('mouseover', function (e) {
                  highlightPark(park);
              });
              div.on('mouseout', function (e) {
                  $.each(np_geo.getLayers(), function (i, l) {
                      l.setStyle(style());
                  });
              });

              var title = $(&quot;&amp;lt;span /&amp;gt;&quot;, {
                  html : &quot;&amp;lt;h2&amp;gt;&amp;lt;span class='glyphicon glyphicon-globe inverse'&amp;gt;&amp;lt;/span&amp;gt;  &quot; + park + &quot; National Park&amp;lt;/h2&amp;gt;&quot;,
                  class : 'parkname-title'
              });

              var fullPhoto = photo.replace(&quot;n.jpg&quot;, &quot;b.jpg&quot;);
	
              var link = $(&quot;&amp;lt;a /&amp;gt;&quot;, {
                  &quot;data-lightbox&quot; : &quot;image-&quot; + idx,
                  &quot;data-title&quot; : park + &quot; (courtesy of Flickr)&quot;,
                  href : fullPhoto
              });

              var img = $(&quot;&amp;lt;img /&amp;gt;&quot;, {
                  src : photo
              });

              link.append(img);
              div.append(link);
              div.append(title);
              gallery_container.append(div);
          });
      });

      list = list.join(' | ');
      $(&quot;#parkslist&quot;).html(list);
      $(&quot;.park-item&quot;).on('click', function (e) {
          e.preventDefault();
          highlightPark(e.target.text);
      })
      $(&quot;#slider&quot;).empty();
      $(&quot;#slider&quot;).append(gallery_container);
      $(&quot;#owl-slider&quot;).owlCarousel({
          margin : 10,
          autoWidth : true,
          stagePadding : 50
      });
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/natl-park-gallery-2.png&quot; alt=&quot;National Park Gallery Image 2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;add style with Bootstrap and dynamic sizing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/natl-park-gallery-4.png&quot; alt=&quot;National Park Gallery Image 4&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;finally, add sidebar content and style&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;http://tannerjt.github.io/natl-park-gallery/&quot;&gt;View Application&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tannerjt/natl-park-gallery&quot;&gt;View Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;What it looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/natl-park-gallery-5.png&quot; alt=&quot;National Park Gallery Image 5&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 23 Apr 2015 00:00:00 -0500</pubDate>
        <link>http://tannergeo.com/blog/site//leaflet/qgis/turfjs/lightbox/flickr_api/data.gov/2015/04/23/National-Parks-Map-and-Image-Gallery.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//leaflet/qgis/turfjs/lightbox/flickr_api/data.gov/2015/04/23/National-Parks-Map-and-Image-Gallery.html</guid>
        
        
        <category>leaflet</category>
        
        <category>qgis</category>
        
        <category>turfjs</category>
        
        <category>lightbox</category>
        
        <category>flickr_api</category>
        
        <category>data.gov</category>
        
      </item>
    
      <item>
        <title>Introduction to Interactive Web Maps Using JavaScript and LeafletJS</title>
        <description>&lt;p&gt;This is an excellent beginning web application for anyone new to JavaScript and/or &lt;a href=&quot;http://leafletjs.com/&quot;&gt;LeafletJS&lt;/a&gt;.  The tutorial was developed by MIT and is easy to understand.  As someone who is grateful to all professors/peers who have shared their knowledge and experience with me over the years, I like to contribute when able.  I have included some modifications for aspiring web developers below that address publishing your application via &lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; as well as &lt;a href=&quot;http://bower.io/&quot;&gt;Bower&lt;/a&gt;, a package manager that makes including frameworks, libraries, and utilities in your projects more convenient for developers.  Altogether, this makes for an excellent start to learning about general web development as well as creating interactive web maps.&lt;/p&gt;

&lt;p&gt;The full MIT tutorial can be found &lt;a href=&quot;http://duspviz.mit.edu/leaflet-js/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;View my application &lt;a href=&quot;http://tannerkj.github.io/MIT-campus-coffee/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h6 id=&quot;adjustments-i-made-when-running-through-this-tutorial&quot;&gt;Adjustments I made when running through this tutorial:&lt;/h6&gt;

&lt;h6 id=&quot;step-1b&quot;&gt;Step 1.b&lt;/h6&gt;

&lt;p&gt;I did not serve locally using Python.  I created a &lt;a href=&quot;https://github.com/tannerkj/MIT-campus-coffee&quot;&gt;GitHub repo&lt;/a&gt; and viewed changes locally using &lt;em&gt;http://localhost/~username/projectname/&lt;/em&gt; before pushing to GitHub.  It is a good standard practice to follow.  Also, you then have the ability to publish your webpages using GitHub Pages.&lt;/p&gt;

&lt;h6 id=&quot;step-1d&quot;&gt;Step 1.d&lt;/h6&gt;

&lt;p&gt;I used &lt;a href=&quot;http://bower.io/&quot;&gt;Bower&lt;/a&gt;, as mentioned above, to add Leaflet to my project.  It would be prudent to become familiar with Bower and its dependencies since many common packages can be easily installed with its use.  Dependencies for Bower include &lt;a href=&quot;https://nodejs.org/&quot;&gt;Node and npm&lt;/a&gt; and &lt;a href=&quot;http://git-scm.com/&quot;&gt;Git&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Using command line, inside my working directory, I entered the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bower install leaflet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will notice a new folder in your working directory called &lt;em&gt;bower_components&lt;/em&gt; that contains the necessary Leaflet files for your project.  Therefore, the Leaflet CSS link in the &lt;em&gt;head&lt;/em&gt; section of your index.html would be referenced as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;link rel=&quot;stylesheet&quot; href=&quot;./bower_components/leaflet/dist/leaflet.css&quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the Leaflet JavaScript link at the bottom of the &lt;em&gt;body&lt;/em&gt; section would be referenced as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;script src=&quot;./bower_components/leaflet/dist/leaflet.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;View my application &lt;a href=&quot;http://tannerkj.github.io/MIT-campus-coffee/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;View my source code &lt;a href=&quot;https://github.com/tannerkj/MIT-campus-coffee&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/MIT_campus_coffee.png&quot; alt=&quot;MIT-campus-coffee&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 09 Apr 2015 00:00:00 -0500</pubDate>
        <link>http://tannergeo.com/blog/site//leaflet/github/bower/development/beginner/2015/04/09/Introduction-To-Interactive-Web-Maps-Using-JavaScript-and-LeafletJS.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//leaflet/github/bower/development/beginner/2015/04/09/Introduction-To-Interactive-Web-Maps-Using-JavaScript-and-LeafletJS.html</guid>
        
        
        <category>leaflet</category>
        
        <category>github</category>
        
        <category>bower</category>
        
        <category>development</category>
        
        <category>beginner</category>
        
      </item>
    
      <item>
        <title>Visualizing Percentage of Storms Resulting in Injuries/Deaths by State (1996 - 2014)</title>
        <description>&lt;p&gt;Starting in 1996, &lt;a href=&quot;http://www.ncdc.noaa.gov/stormevents/pd01016005curr.pdf&quot;&gt;National Weather Service directive 10-1605&lt;/a&gt; mandated the reporting of 48 different types of weather events and their effects on the impacted community.  This tutorial aims to create a visualization focusing on the percent of storms resulting in injuries/deaths reported by state, and to identify which states 
have been more fortunate than others.  The data is provided by &lt;a href=&quot;http://www.ncdc.noaa.gov/stormevents/ftp.jsp&quot;&gt;NOAA&lt;/a&gt; in .csv files organized by individual year.  This tutorial requires a working knowledge of the command line, &lt;a href=&quot;http://postgis.net/&quot;&gt;PostgreSQL/PostGIS&lt;/a&gt;, &lt;a href=&quot;http://www.gdal.org/&quot;&gt;GDAL/ogr2ogr&lt;/a&gt;, &lt;a href=&quot;http://leafletjs.com/&quot;&gt;LeafletJS&lt;/a&gt;, Python, and JavaScript.&lt;/p&gt;

&lt;p&gt;The first step is to create the tables in PostgreSQL and load the data.  A new database is created and the following code executed to enable PostGIS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE EXTENSION postgis;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, the code to generate the table is executed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TABLE storm_events (
	state varchar(20),
	year smallint,
	state_fips smallint,	
	event_type varchar(50),
	injuries_direct smallint,
	injuries_indirect smallint,
	deaths_direct smallint,
	deaths_indirect smallint
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Included are the attributes we need for this application, along with additional fields we may opt to use in the future for further analysis.&lt;/p&gt;

&lt;p&gt;To create a surrogate primary key called id for this table, the following code is executed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALTER TABLE storm_events ADD COLUMN id SERIAL PRIMARY KEY;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To load the data into the table, several options are available.  One option, though not the most efficient, is outlined below:&lt;/p&gt;

&lt;p&gt;1) A python script is written to read from each year’s .csv file in our details_raw folder and creates newly formatted .csv files, excluding the unnecessary data.  It is then executed in the appropriate directory using the command line.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# data_loader.py
# reads .csv files in ./details_raw
# creates new .csv files in a separate directory

import os, csv

in_dir = &quot;./details_raw&quot;

def filter_fields(f):
	with open(root + &quot;/&quot; + f, &quot;rb&quot;) as source:
    	reader = csv.DictReader(source)
    	outf = root + &quot;/formatted/&quot; + f
    	with open(outf, &quot;wb&quot;) as result:
      		writer = csv.DictWriter(result, fieldnames = ['STATE', 'YEAR','STATE_FIPS', 
      		'EVENT_TYPE', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT'])
        	print (&quot;Writing&quot;, outf)
        	writer.writeheader()
        	for row in reader:
            	writer.writerow({'STATE' : row['STATE'],
                             	'YEAR' : row['YEAR'],
                             	'STATE_FIPS' : row['STATE_FIPS'],
                             	'EVENT_TYPE' : row['EVENT_TYPE'],
                             	'INJURIES_DIRECT' : row['INJURIES_DIRECT'],
                             	'INJURIES_INDIRECT' : row['INJURIES_INDIRECT'],
                             	'DEATHS_DIRECT' : row['DEATHS_DIRECT'],
                             	'DEATHS_INDIRECT' : row['DEATHS_INDIRECT']})

for root, dirs, files in os.walk(in_dir):
	for f in files:
    	filter_fields(f)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2) Using postgres command line tools for each csv file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;psql -U postgres

\connect geodata
\copy storm_events from '/Path to csv' delimiter ',' csv header
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, to create views that allow us to analyze the data you have several options but below are a few simple ones that isolate the info we are interested in.  They also allow room for growth should you choose to add aggregates of other attributes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW injuries_deaths AS
SELECT storm_events.state, storm_events.state_fips,
count(*) AS count
FROM storm_events
WHERE storm_events.injuries_direct &amp;lt;&amp;gt; 0 OR
storm_events.injuries_indirect 	&amp;lt;&amp;gt; 0 OR
storm_events.deaths_direct &amp;lt;&amp;gt; 0 OR
storm_events.deaths_indirect &amp;lt;&amp;gt; 0
GROUP BY storm_events.state, state_fips
ORDER BY storm_events.state;

CREATE VIEW state_totals AS
SELECT total.state, total.state_fips,
count(*) AS count
FROM (SELECT storm_events.state,
       storm_events.year,
       storm_events.state_fips,
       storm_events.event_type,
       storm_events.injuries_direct,
       storm_events.injuries_indirect,
       storm_events.deaths_direct,
       storm_events.deaths_indirect
      FROM storm_events) total
GROUP BY total.state, total.state_fips
ORDER BY total.state;

CREATE VIEW map_storm_data AS
(SELECT s.state, s.state_fips, id.count injuries_death_count, s.count 
total_storms_count, 
round(((id.count::float/s.count::float)*100)::numeric,2) percent
FROM injuries_deaths id
LEFT JOIN state_totals s ON id.state = s.state);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A simple query to validate the data and identify the states with the highest percentage of storms resulting in injury/death:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT *
FROM map_storm_data
ORDER BY percent DESC;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To generate a shapefile we can use to visualize our data,  download from TIGER the shapefile needed, and then use command line to convert it to the desired projection and import it into postgres.  In this case, I entered:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shp2pgsql -s 4269:4326 tl_2014_us_state  public.states | psql -h 
localhost -d geodata -U postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now to create a single view that includes the desired calculations along with the associated state geometry data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW state_storm_percent AS
SELECT a.name, b.state_fips, b.injuries_death_count,
b.total_storms_count, b.percent, ST_Simplify(a.geom, 0.005) as geom
FROM states a
LEFT JOIN map_storm_data b
ON a.statefp::int = b.state_fips;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For constructing the application, we will use Leaflet and GeoJSON.  To do this, we will export the map as a shapefile, and use &lt;a href=&quot;http://www.gdal.org/&quot;&gt;GDAL&lt;/a&gt; to convert it to GeoJSON.&lt;/p&gt;

&lt;p&gt;The following code is executed in the command line for PostgreSQL to generate the shapefile and then convert to geojson:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pgsql2shp -f storm_events -h localhost -p 5432 -u postgres -P password
geodata public.state_storm_percent;

ogr2ogr -f GeoJSON storm_events.geojson storm_events.shp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create the map for our application, we will use the same process as described in this &lt;a href=&quot;http://leafletjs.com/examples/choropleth.html&quot;&gt;Leaflet tutorial&lt;/a&gt;.  The end result is a thematic map with a legend and an info window describing the data as you select each state.&lt;/p&gt;

&lt;p&gt;View application &lt;a href=&quot;http://tannerkj.github.io/NOAA_Storm_Events/index.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Thu, 19 Mar 2015 00:00:00 -0500</pubDate>
        <link>http://tannergeo.com/blog/site//postgis/leaflet/noaa/postgresql/2015/03/19/Visualizing-Percentage-of-Storms-Resulting-in-Injury-or-Death-by-State.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//postgis/leaflet/noaa/postgresql/2015/03/19/Visualizing-Percentage-of-Storms-Resulting-in-Injury-or-Death-by-State.html</guid>
        
        
        <category>postgis</category>
        
        <category>leaflet</category>
        
        <category>noaa</category>
        
        <category>postgresql</category>
        
      </item>
    
      <item>
        <title>A Brief Look At Vector Tiles</title>
        <description>&lt;p&gt;This post is a very ‘basic’ look into vector tiles.  Feel free to make improvements to this post through &lt;a href=&quot;https://github.com/tannerjt/blog_posts/blob/master/vector_tiles.md&quot;&gt;github&lt;/a&gt;.  Also, please use the links throughout this document to understand more about vector tiles.&lt;/p&gt;

&lt;p&gt;At ESRI’s 2015 Developer Conference, the plan to &lt;a href=&quot;http://video.esri.com/watch/4215/smart-mapping-with-vector-map-tiles&quot;&gt;implement vector tiles&lt;/a&gt; into their mapping platform was announced.  Most traditional GIS analysts and developers have created &lt;a href=&quot;http://resources.arcgis.com/en/help/main/10.2/index.html#//001700000189000000&quot;&gt;tile caches&lt;/a&gt; or &lt;a href=&quot;https://developers.arcgis.com/javascript/jsapi/arcgistiledmapservicelayer-amd.html&quot;&gt;consumed tiled map service layers&lt;/a&gt; to implement complex geometries with &lt;a href=&quot;http://www.usgs.gov/faq/categories/9860/3604&quot;&gt;more efficiency&lt;/a&gt; as part of their business processes.  To most of the industry (I’m assuming), &lt;a href=&quot;http://wiki.openstreetmap.org/wiki/Vector_tiles&quot;&gt;vector tiles&lt;/a&gt; is a new concept that offers many advantages over traditional formats.  Although the concept is similar to raster tiles, vector tiles return the actual features which creates the ability to manipulate the style and symbology of features dynamically.  This can be done, all while benefiting from the efficiency of only needing to load tiles within the current extent and not the entirety of a features geometry and attributes.  Also, vector tiles also benefit over feature services because the geometry is essentially cached and not needed to be dynamically created on the server in most cases.&lt;/p&gt;

&lt;p&gt;It is important to know that although vector tiles are new to Esri’s platform, they are already present in the geospatial community elsewhere.  &lt;a href=&quot;https://www.mapbox.com/&quot;&gt;Mapbox&lt;/a&gt;  has &lt;a href=&quot;https://www.mapbox.com/blog/vector-tiles/&quot;&gt;developed a standard for an open source vector tile format&lt;/a&gt; and uses it at the core of &lt;a href=&quot;https://www.mapbox.com/tilemill/&quot;&gt;tilemill&lt;/a&gt;.  In fact, &lt;a href=&quot;https://www.mapbox.com/blog/vector-tile-adoption/&quot;&gt;Esri has adopted&lt;/a&gt; this specification in it’s implementation of vector tiles.  If you look at the vector tile spec defined by mapbox, you will also see various &lt;a href=&quot;https://github.com/mapbox/vector-tile-spec/wiki/Implementations&quot;&gt;applications and tools that implement it&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Within &lt;a href=&quot;https://developers.arcgis.com/javascript/jsapi/&quot;&gt;Esri’s current JavaScript API&lt;/a&gt;, I don’t see support for vector tiles but I’m sure the next release will include it.  OpenLayers  does support &lt;a href=&quot;http://openlayers.org/en/v3.3.0/apidoc/ol.source.TileVector.html&quot;&gt;a TileVector&lt;/a&gt; format, as illustrated in &lt;a href=&quot;http://openlayers.org/en/v3.3.0/examples/tile-vector.html&quot;&gt;this example&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are various tile-generation libraries available for creating vector tiles, and according to Esri’s announcement, a toolbox will be available directly though their Desktop (or Pro) software.  One nice example I found was created by &lt;a href=&quot;https://github.com/glob3mobile&quot;&gt;Glob3 Mobile&lt;/a&gt; called &lt;a href=&quot;https://github.com/glob3mobile/mmt-vector-tiles&quot;&gt;mmt-vector-tiles&lt;/a&gt; which can generate vector tiles directly from a PostGIS database.  They also have a &lt;a href=&quot;http://wb.glob3mobile.com/vl/index_lux.html&quot;&gt;good example&lt;/a&gt; posted of this in action with OpenLayers 3.  If you look at the &lt;a href=&quot;http://igosoftware.dyndns.org:8000/vectorial/lux_buildings_LEVELS_12-18_MERCATOR/GEOJSON/&quot;&gt;file stucture&lt;/a&gt; of their tiles, it looks similar to what is shown in &lt;a href=&quot;http://video.esri.com/watch/4215/smart-mapping-with-vector-map-tiles&quot;&gt;Esri’s presentation&lt;/a&gt; although I’m not certain it adheres to the same specification.&lt;/p&gt;

&lt;p&gt;If you haven’t heard of vector tiles before, I hope this post has been useful!&lt;/p&gt;

</description>
        <pubDate>Sun, 15 Mar 2015 00:00:00 -0500</pubDate>
        <link>http://tannergeo.com/blog/site//vector_tiles/formats/2015/03/15/A-Brief-Look-At-Vector-Tiles.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//vector_tiles/formats/2015/03/15/A-Brief-Look-At-Vector-Tiles.html</guid>
        
        
        <category>vector_tiles</category>
        
        <category>formats</category>
        
      </item>
    
      <item>
        <title>Visualizing February 2015 Weather Data with Turf.js</title>
        <description>&lt;p&gt;February was a particularly cold month across most of the United States.  For us here in Oregon, we’ve been enjoying warm weather… but most further east have had record lows and heavy snow.  I’ve been interested in finding new ways to visualize this data.  I’ll be using &lt;a href=&quot;http://cdo.ncdc.noaa.gov/qclcd/QCLCD?prior=N&quot;&gt;NOAA’s quality controlled logical climatological data&lt;/a&gt; to load weather station data across the US into a Postgres database.  From there I hope to generate TIN’s using &lt;a href=&quot;http://turfjs.org/&quot;&gt;turf.js&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The initial hurdle is getting the stations and readings into our database.  Our file from NOAA has the following pipe delineated headers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WBAN|WMO|CallSign|ClimateDivisionCode|ClimateDivisionStateCode|ClimateDivisionStationCode|Name|State|Location|Latitude|Longitude|GroundHeight|StationHeight|Barometer|TimeZone
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I don’t need all of these, just the WBAN (unique id of station), State, Location, Latitude and Longitude.  Let’s create the initial table:&lt;/p&gt;

&lt;p&gt;Enable postgis on database:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE EXTENSION postgis;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create table for stations:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TABLE weather_stations (
    WBAN integer,
    State char(2),
    Location varchar(150),
    Latitude decimal(9,6) NOT NULL,
    Longitude decimal(9,6) NOT NULL
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I deleted the erroneous columns from the original file for easy import.  From psql command line, I ran the &lt;a href=&quot;http://stackoverflow.com/questions/2987433/how-to-import-csv-file-data-into-a-postgres-table&quot;&gt;following command&lt;/a&gt; to bring the stations into postgres:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;\copy weather_stations FROM 'csv location' DELIMITER '|' CSV HEADER;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add an auto incremented primary key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALTER TABLE weather_stations ADD COLUMN id SERIAL PRIMARY KEY;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;http://postgis.net/docs/AddGeometryColumn.html&quot;&gt;Add geometry column&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT AddGeometryColumn ('public', 'weather_stations', 'geom', 4326, 'POINT', 2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, let’s create actual points from our Latitude and Longitude fields:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;UPDATE weather_stations SET
geom = ST_SetSRID(ST_MakePoint(Longitude, Latitude),4326)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we look at our data in QGIS, we see this now!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/points_qgis.png&quot; alt=&quot;qgis screenshpt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that the stations are in our database, we need to import our weather station readings.  The process will be similar to what we accomplished getting the stations in.  The readings are comma delineated and contain information or daily recordings.  The initial format looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WBAN,YearMonthDay,Tmax,TmaxFlag,Tmin,TminFlag,Tavg,TavgFlag,Depart,DepartFlag,DewPoint,DewPointFlag,WetBulb,WetBulbFlag,Heat,HeatFlag,Cool,CoolFlag,Sunrise,SunriseFlag,Sunset,SunsetFlag,CodeSum,CodeSumFlag,Depth,DepthFlag,Water1,Water1Flag,SnowFall,SnowFallFlag,PrecipTotal,PrecipTotalFlag,StnPressure,StnPressureFlag,SeaLevel,SeaLevelFlag,ResultSpeed,ResultSpeedFlag,ResultDir,ResultDirFlag,AvgSpeed,AvgSpeedFlag,Max5Speed,Max5SpeedFlag,Max5Dir,Max5DirFlag,Max2Speed,Max2SpeedFlag,Max2Dir,Max2DirFlag
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously, we are only interested in the WBAN (station id), YearMonthDay, Tmax, Tmin and Tavg.  These will give us the ability to map low, high and average daily temperatures.  I went ahead and removed the other columns from the initial dataset.  Let’s create the table in postgres.  I used varchar for the temperatures because of some string M values which equal missing… we can filter these out later.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TABLE readings_feb (
    WBAN integer,
    YearMonthDay integer NOT NULL,
    Tmax varchar(5),
    Tmin varchar(5),
    Tavg varchar(5)
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Import csv file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;\copy readings_feb FROM 'D:\TannerGeo\WeatherTinTurf\data_raw\QCLCD201502\201502daily.csv' DELIMITER ',' CSV HEADER;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, add a primary key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALTER TABLE readings_feb ADD COLUMN id SERIAL PRIMARY KEY;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To visualize the data in QGIS, I created a special view with just the low temp.  Notice we’ve casted the &lt;a href=&quot;http://dba.stackexchange.com/questions/3429/how-can-i-convert-from-double-precision-to-bigint-with-postgresql&quot;&gt;tmin to int&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW feb_weather AS
SELECT a.id, a.wban, a.yearmonthday, a.tmin::int, b.state, b.location, b.geom
FROM readings_feb a
LEFT JOIN weather_stations b
ON a.wban = b.wban
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By adding a filter for just February 1st in QGIS and symbolizing by temperature, the cold weather trends become apparent.  The mid-west and northeast have bone chilling lows while the west coast and southern US had relatively high temperatures.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/temp_feb01.png&quot; alt=&quot;February Map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To incorporate the data into our mapping application, it’s preferable that we work with a data format suited to our needs.  I’ll be using &lt;a href=&quot;http://geojson.org/&quot;&gt;geojson&lt;/a&gt;, mostly because this format will work best with &lt;a href=&quot;http://turfjs.org/static/docs/global.html#GeoJSON&quot;&gt;turf.js&lt;/a&gt;.  We could &lt;a href=&quot;http://www.postgresonline.com/journal/archives/267-Creating-GeoJSON-Feature-Collections-with-JSON-and-PostGIS-functions.html&quot;&gt;build a query&lt;/a&gt; to generate this straight from postgres, but I’d prefer to just use &lt;a href=&quot;http://www.bostongis.com/pgsql2shp_shp2pgsql_quickguide.bqg&quot;&gt;pgsql2shp&lt;/a&gt; and then &lt;a href=&quot;http://www.gdal.org/ogr2ogr.html&quot;&gt;GDAL’s ogr2ogr&lt;/a&gt; to convert to geojson.&lt;/p&gt;

&lt;p&gt;Create shapefile using pgsql command line tools:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pgsql2shp -f weather_02012015 -h localhost -p 5432 -u username -P password geodata &quot;SELECT * FROM feb_weather WHERE yearmonthday = 20150201&quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Convert to GeoJSON using &lt;a href=&quot;http://www.gisinternals.com/query.html?content=filelist&amp;amp;file=release-1800-x64-gdal-1-11-1-mapserver-6-4-1.zip&quot;&gt;GDAL&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ogr2ogr -f GeoJSON ../geojson/02012015.geojson 02012015.shp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our output file is 287KB could probably be smaller since we don’t necessarily need all of the fields to make the TIN map.&lt;/p&gt;

&lt;p&gt;The only dependencies I know of that we need to create the map are turf.js and leaflet.  I chose to use bower to install these dependencies:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bower install turf
bower install leaflet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For bringing in the geojson, I’m just going to assign the object to a variable and place it in a seperate .js file.   For future reference though, &lt;a href=&quot;https://github.com/calvinmetcalf&quot;&gt;calvinmetcalf&lt;/a&gt; has created a great helper for leaflet called the &lt;a href=&quot;https://github.com/calvinmetcalf/leaflet-ajax&quot;&gt;leaflet-ajax&lt;/a&gt; library for working with geojson files.&lt;/p&gt;

&lt;p&gt;We now have a .js file called 02012015.js with the geojson assigned to the variable feb01.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var feb01 = {
    type&quot;: &quot;FeatureCollection&quot;,
	&quot;crs&quot;: { &quot;type&quot;: &quot;name&quot;, &quot;properties&quot;: { &quot;name&quot;: &quot;urn:ogc:def:crs:OGC:1.3:CRS84&quot; } },
	...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I want to first make sure I can create the TIN’s.  To test, I use turf.js to build and add them to the map with no fill.  By passing in TMIN, turf generates Z values that we can then use to generate our fill colors.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var tin = turf.tin(feb01, 'TMIN');
var geojson = L.geoJson(tin, {
	style : {
		weight : 1
	}
});
geojson.addTo(map);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our result is a visually interesting graphic:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/tin_nofill.png&quot; alt=&quot;tin image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For our fill colors, I’m going to use a library I built called &lt;a href=&quot;https://github.com/tannerjt/classybrew&quot;&gt;classybrew&lt;/a&gt;.  It uses the natural breaks (Jenks) method of statistical classification and applies colors based on Cynthia Brewers color palettes.  I added a new diverging color palette for red-yellow-blue.  This should give us a good thematic perspective of color.&lt;/p&gt;

&lt;p&gt;To stylize the map, we first need to initialize and setup or statistical breaks and color palette based on our minimum temperature values.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// create classification for colors
var brew = new classyBrew();
var values = []; // tmin values
for( var i = 0; i &amp;lt; feb01.features.length; i++) {
	values.push(feb01.features[i].properties['TMIN']);
}
brew.setSeries(values);
brew.setNumClasses(6);
brew.classify();
brew.setColorCode(&quot;BuYlRd&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have the classybrew object setup, we can request a color code based on a provided value with &lt;code&gt;getColorInRange(value)&lt;/code&gt;.  We’ll use this function to return a color based on the average of minimum temperature from each station that makes up our triangles.  This can be done during instantiation of our geojson layer.  First, however we need to create our TIN from our points using &lt;code&gt;turf.tin(geojson, z value)&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var tin = turf.tin(feb01, 'TMIN');
var geojson = L.geoJson(tin, {
	style : function (f) {
		return {
			weight : 1,
			fillColor : (function () {
				return brew.getColorInRange((f.properties.a + f.properties.b + f.properties.c) / 3.0, true);
			}()),
			fillOpacity : 0.85
		}
	}
});
geojson.addTo(map);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our result is rather captivating:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/tin_fill.png&quot; alt=&quot;Weather TIN Map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we look at a map from the NOAA National Climatic Data Center, we can see our map aligns with their model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tannergeo.com/images/blog/screenshots/noaa_temp.png&quot; alt=&quot;NOAA NCDC Map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I think this is all I was looking to do for now.  The output is visually interesting and shows how powerful turf.js is with very little code.  I’d like to take this further and make a more in depth application with the data and turf.js.  Look for more to come in the future, and thanks for following along!&lt;/p&gt;
</description>
        <pubDate>Sun, 01 Mar 2015 00:00:00 -0600</pubDate>
        <link>http://tannergeo.com/blog/site//turfjs/leaflet/postgresql/postgis/noaa/2015/03/01/Visualizing-February-2015-Weather-Date-with-Turf.js.html</link>
        <guid isPermaLink="true">http://tannergeo.com/blog/site//turfjs/leaflet/postgresql/postgis/noaa/2015/03/01/Visualizing-February-2015-Weather-Date-with-Turf.js.html</guid>
        
        
        <category>turfjs</category>
        
        <category>leaflet</category>
        
        <category>postgresql</category>
        
        <category>postgis</category>
        
        <category>noaa</category>
        
      </item>
    
  </channel>
</rss>
